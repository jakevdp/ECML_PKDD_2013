\documentclass{llncs}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
%\usepackage{hyperref}
\usepackage[authoryear,round]{natbib}
\usepackage{xcolor}
\usepackage{xspace}

\pagestyle{headings}

\newcommand{\sklearn}{\textit{scikit-learn}\xspace}

% working title, please change
\title{Python as a machine learning language:
       experiences from the scikit-learn project}

\author{Mathieu Blondel\inst{1} \and
        Lars Buitinck\inst{2} \and
        Gilles Louppe\inst{3} \and
        Jake Vanderplas\inst{4}}

\institute{Graduate School of System Informatics, Kobe University \and
           Informatics Institute, University of Amsterdam \and
           Department of EE \& CS, University of Li√®ge \and
           Astronomy Department, University of Washington}

% Dutch name sorting hack as per http://tex.stackexchange.com/a/40750/2806
\DeclareRobustCommand{\VAN}[3]{#2}

\begin{document}

\maketitle

\begin{abstract}
The \sklearn project is an increasingly popular open source machine
learning library designed to be simple and efficient, accessible to non-experts,
and reusable in various contexts. In this paper, we present and discuss our
design choices for the application programming interface (API) of the project.
In particular, we describe the simple and elegant interface shared by all
learning and processing units in the library and then discuss its advantages in
terms of composition and reusability. The paper also comments on implementation
details specific to the Python ecosystem and analyzes obstacles faced by users
and developers of the library.
\end{abstract}

\noindent \textcolor{red}{TODO: Focus on the experience we accumulated (general remark)}\\
\textcolor{red}{TODO: Motivate our design choices.}

\section{The scikit-learn project}

The \sklearn project \citep{pedregosa2011} is an open source
machine learning library for the Python programming language. The ambition of
the project is to provide efficient and well-established machine learning tools
within a programming environment that is accessible to non-machine learning
experts and reusable in various scientific areas. In itself the project isn't
a novel domain-specific language, but a library that provides machine
learning idioms to a general-purpose high-level language. Among other things,
it includes classical learning algorithms, model selection tools and
preprocessing procedures.

The project is a programming library, i.e. a collection of classes -- in the
object-oriented sense -- that users import into Python programs. Using \sklearn
therefore requires basic Python programming knowledge. No command-line
interface, let alone a graphical user interface, is offered for non-programmer
users. Instead, interactive use is made possible by the Python interactive
interpreter, and its enhanced replacement IPython \citep{perez2007ipython},
which offer a \textsc{matlab}-like working environment specifically designed for
scientific use.

From its core, the library has been designed to tie in with the set of numeric
and scientific packages centered around the NumPy and SciPy libraries. NumPy
\citep{vanderwalt2011} augments Python with a numeric array datatype and
associated array computing primitives while SciPy \citep{varoquaux2013scipy}
extends it further by making common numeric operations available, either by
implementing those in Python/NumPy or by wrapping existing
C/C{}\verb!++!/Fortran implementations. In a similar vein, \sklearn extends
Python with a set of composable machine learning operations.

Started in 2007, \sklearn is developed by an international team of a dozen of
core developers, mostly researchers from various fields (e.g., machine learning,
natural language processing, or neuroscience). The project also greatly benefits
from many occasional contributors proposing small code or documentation fixes or
improvements. Development proceeds on GitHub\footnote{\url{https://github.com/scikit-learn}},
a platform which greatly facilitates this kind of
collaboration. Because of the large number of developers, emphasis is
put on keeping the project maintainable. In particular, code must follow
specific quality guidelines, such as style consistency and unit-test coverage.
In addition, complete documentation and examples are required for all features.
Finally, additions to the code base must pass code review by at least two
developers not involved in the implementation of the proposed addition.

Within the past few years, the project has proved valuable to an ever increasing
user base. Its popularity can be gaged from various indicators such as hundreds
of citations in scientific publications, successes in various machine learning
challenges (e.g., Kaggle), or significant statistics derived from our
repositories and mailing lists.  At the time of writing, the project is watched
by 1309 people and forked 667 times on GitHub, and the mailing-list receives
more than 300 mails per month. The version control logs for the development
version
% ddaa494c116e3c16bf032003c5cccbed851733d2
also count 183 contributors to the project (after manual deduplication).

In a previous paper \citep{pedregosa2011}, \sklearn was briefly presented and
benchmarked against several competitors. In this paper, we instead focus on an
in-depth discussion and analysis of the design choices used in building the
library. First, in section \ref{sec:core-api}, we present the central application
programming interface (API) and then describe, in section \ref{sec:advanced-api},
advanced API mechanisms built on this core API. Section
\ref{sec:implementation} describes implementation trade-offs and analyzes
limitations of the technologies we rely on, while section
\ref{sec:comparison} then compares \emph{scikit-learn} to other major projects in
terms of API. Finally, conclusions and directions of future works are included
in section \ref{sec:conclusions}.

\section{Core API}

\label{sec:core-api}

In this section, we discuss the general principles of the application
programming interface (API) of \sklearn. We first specify the representation of
data and then proceed by describing the core interface of the library.

All objects within \sklearn share a uniform common basic API consisting of three
complementary interfaces: an \textit{estimator} interface for building and
fitting objects, a \textit{predictor} interface for making predictions and a
\textit{transformer} interface for converting data. As much as possible, our
design choices have been guided so as to avoid the proliferation of framework
code. We try to adopt simple conventions and to limit to a minimum the number of
methods an object must implement. The API is designed to adhere to the following
broad principles:

\begin{description}
  \item[Consistency.]
       All objects (basic or composed) share a consistent interface composed of
       a limited set of methods. This interface is documented in a consistent
       manner for all objects.
  \item[Inspection.]
       Constructor parameters and parameter values determined by learning
       algorithms are stored and exposed as public attributes.
  \item[Non-proliferation of classes.]
       Learning algorithms are the only objects to be of custom classes.
       Datasets and hyperparameter names and values are represented as standard
       Python, NumPy or SciPy classes rather than new ones whenever feasible.
       This keeps \sklearn easy to use and easy to combine with other libraries.
       % XXX do we need to elaborate more on this, or do we expect everyone
       % to be an experienced Java/adapter pattern hater?
  \item[Composition.]
       Many machine learning tasks are expressible
       as sequences or combinations of transformations to data.
       Some learning algorithms are also naturally viewed
       as meta-algorithms parameterized on other algorithms.
       Whenever possible, those algorithms are composed from
       existing building blocks.
  \item[Sensible defaults.]
       Whenever an operation requires a user-defined parameter,
       a default and appropriate value is defined by the library.
       The default value should cause the operation to be performed
       in a sensible way (giving a baseline solution for the task at hand).
\end{description}

\subsection{Data representation}
\label{sec:arrays}

In most machine learning tasks, data is modeled as a set of variables.  For
example, in a classification or regression task, the goal is to find a mapping
between input variables $X_1, ..., X_p$, called features, and
some output variable $Y$. A sample is then defined as a tuple of values $([x_1,
..., x_p]^\mathrm{T}, y)$ of these variables and a dataset as a collection of
such samples.  As such, matrices of numerical values constitute a natural and
widely used representation in machine learning. In this view, each row in the
matrix then corresponds to a sample of the dataset and each column to one of the
variables of the problem.

In \sklearn, we chose a representation of data that is as close as
possible to the matrix representation: datasets are encoded as NumPy
multidimensional arrays for dense data and as SciPy sparse matrices for sparse
data. While those may seem rather unsophisticated data representations when
compared to more object-oriented constructs, such as those used by
Weka \citep{hall2009weka}, they bring the prime advantage of allowing us to rely
% Lars: "orders of magnitude faster" is an implementation detail
on all NumPy and SciPy vector operations while allowing us at the same time to
keep the code short and readable.  This design choice has also been motivated by
the fact that, given their pervasiveness in many other scientific Python
packages, many scientific users of Python are already familiar with NumPy dense
arrays and SciPy sparse matrices, hence helping them to familiarize with the
project.  From a practical point of view, these formats also provide a bunch of
data loading and conversion tools which make them very easy to use in many
contexts. Moreover, for tasks where the inputs are text files or semi-structured
objects, we provide \textit{vectorizer} objects that efficiently convert such
data to the NumPy or SciPy formats.

The public API is oriented towards batch processing rather than processing
single samples per API call. While classification and regression algorithms can
indeed make predictions for single samples, \sklearn objects are not optimized
for this use case. (The few online learning algorithms implemented are intended
to take minibatches.) Batch processing makes optimal use of NumPy and SciPy by
preventing the overhead inherent to Python function calls or due to per-element
dynamic type checking. Although this might seem to be an artifact of the Python
language, and therefore an implementation detail that leaks into the API, we
argue that APIs should be designed so as not to tie a library to a suboptimal
implementation strategy. As such, batch processing enable fast implementations
in lower-level languages (where memory hierarchy effects and the possibility of
internal parallelization come into play).


\subsection{Estimators}

The \textit{estimator} interface is at the core of the
library. It defines instantiation mechanisms of objects and exposes a
\texttt{fit} method for learning a model from training data.  All supervised and
unsupervised learning algorithms (e.g., for classification, regression or
clustering) are offered as objects implementing that interface. Machine
learning tasks like feature extraction, feature selection or dimensionality
reduction are also provided as estimators.

The constructor of an estimator object takes as input a set of named constant
hyper-parameters. At initialization, all given parameters are attached to the
estimator instance as public attributes and serve to determine the estimator
behavior (e.g., the $C$ constant in SVMs). Parameters set as public attributes
are also used for introspection purposes, e.g., in model selection routines. To
ease their use, default hyper-parameter values are provided for all built-in
estimators. These default values are set to be relevant in most common
situations in order to make estimators as efficient as possible
\textit{out-of-box} for non-experts. Finally, the constructor of an estimator
never performs any actual learning (this is the role of the \texttt{fit} method,
see below).  Its only goal is to set the parameter values and freeze the
behavior of the estimator.

The estimator interface defines a \texttt{fit} method. This method is called
with training data (e.g., supplied as two arrays \texttt{X\_train} and
\texttt{y\_train} in supervised learning estimators). Its task is to run a
learning algorithm and to determine model- specific parameters from the training
data and set these as attributes on the estimator object. As a convention, the
parameters learned by an estimator are exposed as public attributes whose name
is suffixed with a trailing underscore (e.g., \texttt{coef\_} for the
learned coefficients of a linear model).

As an example, let us consider a supervised learning task using logistic regression.
Given the API defined above, solving this problem is as simple as:
\begin{verbatim}
    from sklearn.linear_model import LogisticRegression
    clf = LogisticRegression(penalty="l1")
    clf.fit(X_train, y_train)
\end{verbatim}
In this snippet, a \texttt{LogisticRegression} estimator is first initialized by
setting the \texttt{penalty} hyper-parameter to \texttt{"l1"}, hence enabling
$\ell_1$ regularization. Other hyper-parameters (such as \texttt{C}, which
defines the strength of the regularization) are not explicitly defined and are
thus set to the default values. Upon calling \texttt{fit}, the model is then
learned from the training arrays \texttt{X\_train} and \texttt{y\_train}. Since
all estimators share the same interface, using a different learning algorithm is
as simple as replacing the constructor. For example, to build a Random Forest on
the same training data, one would simply have to replace
\texttt{LogisticRegression(penalty="l1")} in the snippet above by \\
\texttt{RandomForestClassifier()}.

In \sklearn, classical learning algorithms are not the only objects
to be implemented as estimators. For example, preprocessing routines (e.g.,
vectorization of text documents, scaling of features, etc) or feature
extraction techniques also implement the same interface. Even those estimators
that are entirely stateless and therefore do not require the \texttt{fit} method
to perform any useful work implement the estimator interface. As we will
illustrate in the next sections, this design pattern is indeed of prime
importance for consistency, composition and parameter validation reasons.

\subsection{Predictors}

The \textit{predictor} interface defines a \texttt{predict}
method taking as input some new data \texttt{X\_test} and producing as output
predictions for \texttt{X\_test}, based on the learned parameters of the
estimator (we call the input to \texttt{predict} ``\texttt{X\_test}'' in order
to emphasize that \texttt{predict} generalizes to new data). In the case of
supervised learning estimators, this method typically returns the predicted
labels or values computed by the model.  Continuing with the previous example,
predicted labels for \texttt{X\_test} can be obtained using the following
snippet:
\begin{verbatim}
    y_pred = clf.predict(X_test)
\end{verbatim}
Some unsupervised learning
estimators may however also implement that interface, as illustrated below:
\begin{verbatim}
    from sklearn.cluster import KMeans
    km = KMeans(n_clusters=10)
    km.fit(X_train)
    clust_pred = km.predict(X_test)
\end{verbatim}
In the above example, a \texttt{KMeans} estimator is first fit on the training
data \texttt{X\_train}. Then the \texttt{predict} method is called on the test
data \texttt{X\_test} in order to retrieve predicted cluster labels.
The \texttt{predict} method in \sklearn thus offers a unified way
to compute predictions with respect to unseen data.

Along with the \texttt{predict} method, predictors may also implement methods
for quantifying the certainty of a prediction. In the case of estimators
implementing linear models, the \texttt{decision\_function} method can be used
to measure the distance of the samples from the discriminating hyperplane. Some
predictors also provide a \texttt{predict\_proba} method which returns
class probabilities for the input samples \texttt{X\_test}.

Finally, predictors should also provide a \texttt{score} function to assess the
performance on some input data. In supervised learning estimators, this method
takes as inputs a dataset \texttt{X\_test} and \texttt{y\_test} and typically
computes the coefficient of determination between \texttt{y\_test} and
\texttt{predict(X\_test)} in the case of classification or the mean squared
error in the case of regression. Not however that, from an API point of view,
the only guideline is that the \texttt{score} method returns a numerical value
such that the higher the better. As such, some unsupervised learning estimators
also exposes a \texttt{score} function, computing for instance the likelihood of
the given data.

\subsection{Transformers}

Since it is common to modify or filter data before feeding it to a learning
algorithm, some estimators in the library implement a \textit{transformer}
interface which defines a \texttt{transform} method. It takes as input some new
data \texttt{X\_test} and yields as output a transformed version of
\texttt{X\_test}. Preprocessing, feature selection and dimensionality reduction
algorithms are all provided as transformers within the library.  In our example,
if one wanted to preprocess the inputs \texttt{X\_train} before fitting the
logistic regression estimator, e.g., by standardizing the feature values, one
could thus simply write:
\begin{verbatim}
    from sklearn.preprocessing import StandardScaler
    scaler = StandardScaler()
    scaler.fit(X_train)
    X_train = scaler.transform(X_train)
\end{verbatim}
Of course, in practice, it is important to apply the same preprocessing to the
test data \texttt{X\_test}. This can easily be done as follows:
\begin{verbatim}
    X_test = scaler.transform(X_test)
\end{verbatim}
Transformers also include a variety of learning algorithms, such as
dimension reduction (PCA, manifold learning techniques), kernel
approximation techniques, and any other approach that can be used to
derive a modified representation of the data.

Finally, let us also note that, the \texttt{fit} method always returns the estimator
it was called on, hence making method chaining possible.  Using this technique,
we can thus rewrite the \texttt{StandardScaler} example above in a single line:
\begin{verbatim}
    X_train = StandardScaler().fit(X_train).transform(X_train)
\end{verbatim}
In addition, \texttt{fit(X\_train).transform(X\_train)} can further
be rewritten as \texttt{fit\_transform(X\_train)}.  Combining the \texttt{fit}
and \texttt{transform} methods into a single method \texttt{fit\_transform}
allows to share computations, which often makes a huge difference in practice.
In the same spirit, \sklearn sometimes provides the
\texttt{fit\_predict} method for combining the \texttt{fit} and \texttt{predict}
methods. This is especially useful for retrieving cluster predictions with
respect to the training data \texttt{X\_train} in clustering algorithms.


\section{Advanced API}

\label{sec:advanced-api}

Building on the core interface introduced in the previous section, we now
present in this section advanced API mechanisms for building meta-estimators,
composing complex estimators and then selecting models.

\subsection{Meta-estimators}

\textcolor{red}{TODO}

\subsection{Pipelines and feature unions}

A distinguishing feature of the \sklearn API is its ability to
compose new estimators from several base estimators. Composition mechanisms can
be used to combine typical machine learning workflows into a single object which
is itself an estimator. Thanks to duck typing, composed
estimators can be used wherever usual estimators can be used. In particular,
they integrate within model selection routines, allowing one to optimize at once
every step of a complex workflow. Composition of estimators can be done in two
ways: either sequentially through \texttt{Pipeline} objects or in a parallel
fashion through \texttt{FeatureUnion} objects.

\texttt{Pipeline} objects are used for chaining multiple estimators into a
single one. This is useful since a machine learning workflow typically involves
a fixed sequence of steps in processing data (e.g., feature extraction,
normalization, feature selection, learning and making predictions). A sequence
of $N$ such steps can be combined into a composite object called a
\texttt{Pipeline}. The first $N-1$ steps must be transformers and the last can
be either a predictor or a transformer. Conceptually, fitting a pipeline to a
training set amounts to the following recursive procedure: i) when only one step
remains, call its \texttt{fit} method; ii) otherwise, fit the first step, use it
to \texttt{transform} the training set and fit the rest of the pipeline with the
transformed data.
The pipeline exposes all the methods the last
estimator in the pipe exposes. That is, if the last estimator is a predictor,
the pipeline can itself be used as a predictor. If the last estimator is a
transformer, then the pipeline is itself a transformer. \textcolor{red}{TODO:
Talk about nested attribute syntax.}

A \texttt{FeatureUnion} object is a transformer combining multiple transformers
into a single one that joins their outputs. From an API point of view, a
\texttt{FeatureUnion} takes as input a list of transformers. Calling
\texttt{fit} on the union is the same as calling \texttt{fit} independently on
each of the transformers. For transforming data, the base transformers are
applied in parallel and their outputs are concatenated end-to-end into a larger
output.

As an example, the following snippet illustrates how to create a complex worflow
computing both linear PCA and kernel PCA components on \texttt{X\_train}
(through a \texttt{FeatureUnion}), selecting the 100 best of those features and
then feeding them to an $\ell_2$-regularized logistic regression model:
\begin{verbatim}
    from sklearn.pipeline import Pipeline
    from sklearn.decomposition import PCA, KernelPCA
    from sklearn.feature_selection import SelectKBest

    feat_union = FeatureUnion([('pca', PCA()),
                               ('kpca', KernelPCA(kernel="rbf"))])

    Pipeline([("pca", feat_union,
              ("feature_selection", SelectKBest(k=100)),
              ("logistic_model", LogisticRegression(penalty="l2"))
    ]).fit(X_train, y_train).predict(X_test)
\end{verbatim}

As the above snippet shows, \texttt{Pipeline} and \texttt{FeatureUnion} can be
combined to create complex objects. In particular, since both are themselves
estimators, they can be used to form nested workflows. \textcolor{red}{TODO:
elaborate a bit more on nested constructions.}

\subsection{Model selection}

\textcolor{red}{TODO: rewrite and briefly describe the API (see user guide)}\\
\textcolor{red}{TODO: describe Scorer API?}\\
\textcolor{red}{TODO: mention that GridSearchCV objects are themselves estimators}

The aforementioned \texttt{Pipeline} object
can be passed as an argument to a model selection algorithm.
Available algorithms at the time of writing include random search
\citep{bergstra2012} and ``grid search''.
The latter, which we shall use in this example,
performs an exhaustive search through a grid formed by the cartesian product
of a set of possible values for each parameter, specified by the user.
In our example, this grid might be given as:

\begin{align*}
         & \textsf{stopwords} \in \{0, 1\}                      \\
  \times & \; \textsf{tf} \in \{\textsf{linear}, \textsf{log}\} \\
  \times & \; n_\textsf{features} \in \{1000, 2000, 5000\}      \\
  \times & \; \textsf{norm} \in \{\ell_1, \ell_2\}              \\
  \times & \; C \in \{10, 100, 1000\}
\end{align*}

At each point in this grid, $k$-fold cross validation is run
to estimate the performance of the estimator according to some measure
(e.g., accuracy, $F_\beta$-score or a host of clustering quality metrics)
and the best performing set of hyperparameters is stored
on the \texttt{GridSearchCV} object, again, as a public attribute.

\section{Implementation}

\label{sec:implementation}

\noindent \textcolor{red}{TODO: Mention small set of dependencies.}\\
\textcolor{red}{TODO: elaborate on use of mixins and duck typing}

scikit-learn is primarily implemented in Python and Cython
\citep{behnel2011cython},
a language that extends Python with static typing
and a compiler that produces C extension modules
for the Python runtime system.
In addition, it includes (modified versions of)
the \textsf{LIBSVM} and \textsf{LIBLINEAR} libraries
used for training support vector machines
and logistic regression models \citep{chang2011libsvm, fan2008}.
These libraries are both written in C{}\verb!++!
and wrapped using Cython modules.

As much as possible is implemented in ``pure'' Python,
to avoid the compile-edit-debug cycle needed for Cython
and to keep the codebase readable for as large an audience as possible.
However, since the speed of the current Python implementation
is not sufficient for numeric programming
and NumPy's vector operations are not appropriate for all use cases
(among other factors, because they often have linear space requirements),
some of the core algorithms have to be implemented in Cython.\footnote{
  The only Python implementation currently supported by scikit-learn
  is the reference implementation, CPython.
  The alternative Python implementation PyPy \citep{bolz2009tracing}
  promises to alleviate this problem by JIT compilation,
  but does not yet interoperate with NumPy and SciPy.
}
Examples include the aforementioned $k$-means implementation,
stochastic gradient descent for linear models
and some graph-based clustering algorithms.

While object-oriented techniques and design patterns
are used throughout scikit-learn,
they mostly serve to facilitate code reuse and are not considered a project goal.
Where possible, the Python principle of ``duck typing'' is exploited
to simplify the implementation and skip the introduction of superfluous classes.
This allows for extensibility and flexibility at the same time:
user-defined estimators that follow scikit-learn conventions
should be usable in pipelines and other composite objects
without them actually inheriting from scikit-learn base classes.

%\section{Weaknesses of the NumPy/SciPy environment}



\textcolor{red}{TODO: The following corresponds to the old weaknesses section. It should be properly rewritten.}

% This needs an introductory paragraph, but the best I could come up with was:
As can be expected, the NumPy/SciPy environment is not completely flawless.

% The following hedge needs to be in here; we don't want to diss our peers
It should be noted that this section is meant
as a summary of challenges faced by scikit-learn development
and an attempt at constructive criticism.
There is considerable overlap between the communities
that develop or contribute to NumPy, SciPy, scikit-learn, and Cython,
so the issues raised here may eventually be resolved
by the scikit-learn developers themselves.

\subsection{Sparse matrix support}

SciPy's handling of sparse matrices is one source of confusion
for scikit-learn developers and users alike.
The \texttt{scipy.sparse} package offers support
for this kind of data structure,
which is crucial to achieving performance and scalability
in the face of high-dimensional inputs
(including, but not limited to, NLP applications).
However, the interface exposed by its classes
deviates from the NumPy array interface, sometimes subtly so,
sometimes entirely.  % XXX is this grammatically correct?
For example, while NumPy arrays have a variable number of dimensions,
so that vectors can be considered 1-d arrays
(without a distinction between row and column vectors),
matrices are 2-d, etc., \texttt{scipy.sparse} matrices are always 2-dimensional,
meaning that special care has to be taken when handling sparse vectors.

The development team has built up sufficient familiarity
with the sparse/array distinction to factor out common patterns
for supporting both sparse and dense input data structures in many estimators.
In other cases, though, sparse matrix support has to implemented
on a per-estimator basis.

\subsection{Parallelism}

At this point in time, support for multi-core programming in Python,
at least in the CPython reference implementation targeted by scikit-learn,
is downright weak.
The interpreter supports multithreading, but due to the ``Global Interpreter Lock'' (GIL),
no two threads in a process may at the same time execute Python code.
It follows that multithreading is not a viable solution
even two embarrassingly parallel problems,
such as fitting and evaluation multiple models in a grid search procedure.
The workaround is to use Python's multiprocessing module,
which emulates threads using multiple operating system-level processes,
but this in turn leads to high memory use and overhead
because of the copying of training and test sets
into the various processes' address spaces.

While Cython makes writing performance-critical code possible
in a language that is very much like Python,
doing so properly still requires some knowledge of C.
One source of trouble has been the various fixed-size integer types
that are used by the Python and NumPy C runtimes,
whose sizes are practically all platform-dependent.

Another problem concerns the mismatch between on the one hand Python functions,
which are first-class runtime objects endowed with methods and metadata
that identify them as ``callable'',
that dynamically check their number and types of arguments,
and that can return any Python type of object,
and compiled C or Cython functions on the other hand,
which exhibit static typing.
Where it may sometimes be convenient for estimators
to take functions as arguments
(e.g., in the case of kernel machine learning,
where one might like to use a custom kernel),
but such usage is not at present possible and may never be.
Passing C or Cython-compiled functions would require an API for the kernel method
to be implemented at the C or Cython level,
only to be used from compiled code.
Passing arbitrary Python functions would be possible,
but performance can be expected to degrade significantly
because of the runtime checks needed for each invocation of the kernel function.
(The current solution in scikit-learn is to let the user pass not a kernel function,
but a precomputed Gram matrix for an entire set of objects.
The linear, polynomial and RBF kernels are treated as special cases in
the \textsf{LIBSVM}-based SVM learner, where they are implemented in C{}\verb!++!.)

\section{Comparison with other packages}

\label{sec:comparison}

\textcolor{red}{TODO: compare with weka, R, vowpal rabbit, ... ?}\\
\textcolor{red}{TODO: emphasize scikit-learn's place in the overall scipy ecosystem, and its role in spurring the development of more basic tools.}\\
\textcolor{red}{TODO: some packages start to adopt our own API.}

Recent years have witnessed a rising interest in machine learning and data mining
with applications in many fields.
With this rise comes a host of machine learning packages
(both open source and proprietary) with which \sklearn competes.

% XXX Is the Julia comparison fair?
% The goal of that language seems to be a bit different:
% to provide a compiler to implement numeric algorithms without vectorized idioms,
% maybe more like Cython than Python...
In comparison to specialized languages for numeric and statistical programming
such as \textsc{matlab}, R \citep{trancon2012r} and Julia \citep{bezanson2012julia},
the use of Python as a host language has the distinct advantage
that it is a mature, \textit{general purpose} language:
it has strong language and standard library support for such tasks as
string handling/text processing, interprocess communication, networking
and many of the other tasks that many machine learning programs
(whether academic or commercial)
will have to perform ``on the side''.
In addition, third-party libraries are available for virtually any task
that modern computer programs routinely perform.

Another package that deserves mention is the Gensim topic modeling toolkit
\citep{rehurek2010gensim}.
Although its stated goal is not machine learning,
it does exemplify a style of API design for scalable processing of ``big data''
that takes a different approach than the comparable models in \sklearn.
Gensim's way of dealing with large datasets is to use algorithms
that have $O(1)$ space complexity and can be updated online,
called \textit{document streaming} by \citeauthor{rehurek2010gensim}.
The API is designed around the Python concept of an \textit{iterable}
(supported in the language by a restricted form of co-routines called
\textit{generators}).
While the text vectorization part of \sklearn
also uses irerables to some extent,
the emphasis is still largely on (mini-)batch processing,
even in the truly stateless, O(1) memory vectorizers
that implement the hashing trick of \citet{weinberger2009}.
This way of processing, as argued earlier in Section~\ref{sec:arrays},
reduces various forms of overhead
and allows users to make use of the abundance of memory in modern machines.
No attempt is made to hide this batch-oriented processing from the user,
allowing control over the amount of memory actually dedicated
to \sklearn algorithms.


\section{Conclusions and future works}
\label{sec:conclusions}

ToDo.

\bibliographystyle{plainnat}
\DeclareRobustCommand{\VAN}[3]{#3}
\bibliography{paper}

\end{document}
