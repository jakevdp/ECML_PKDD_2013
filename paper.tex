\documentclass{llncs}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
%\usepackage{hyperref}
\usepackage[authoryear,round]{natbib}
\usepackage{xcolor}
\usepackage{xspace}

\pagestyle{headings}

\newcommand{\sklearn}{\textit{scikit-learn}\xspace}

% working title, please change
\title{Python as a machine learning language:
       experiences from the scikit-learn project}

\author{Mathieu Blondel\inst{1} \and
        Lars Buitinck\inst{2} \and
        Gilles Louppe\inst{3} \and
        Jake Vanderplas\inst{4}}

\institute{Graduate School of System Informatics, Kobe University \and
           Informatics Institute, University of Amsterdam \and
           Department of EE \& CS, University of Li√®ge \and
           Astronomy Department, University of Washington}

% Dutch name sorting hack as per http://tex.stackexchange.com/a/40750/2806
\DeclareRobustCommand{\VAN}[3]{#2}

\begin{document}

\maketitle

\begin{abstract}
The \sklearn project is an increasingly popular open source machine
learning library designed to be simple and efficient, accessible to non-experts,
and reusable in various contexts. In this paper, we present and discuss our
design choices for the application programming interface (API) of the project.
In particular, we describe the simple and elegant interface shared by all
learning and processing units in the library and then discuss its advantages in
terms of composition and reusability. The paper also comments on implementation
details specific to the Python ecosystem and analyzes obstacles faced by users
and developers of the library.
\end{abstract}

\noindent \textcolor{red}{TODO: Focus on the experience we accumulated (general remark)}\\
\textcolor{red}{TODO: Motivate our design choices.}

\section{The scikit-learn project}

The \sklearn project \citep{pedregosa2011} is an open source
machine learning library for the Python programming language. The ambition of
the project is to provide efficient and well-established machine learning tools
within a programming environment that is accessible to non-machine learning
experts and reusable in various scientific areas. In itself the project isn't
a novel domain-specific language, but a library that provides machine
learning idioms to a general-purpose high-level language. Among other things,
it includes classical learning algorithms, model selection tools and
preprocessing procedures.

The project is a programming library, i.e. a collection of classes -- in the
object-oriented sense -- that users import into Python programs. Using \sklearn
therefore requires basic Python programming knowledge. No command-line
interface, let alone a graphical user interface, is offered for non-programmer
users. Instead, interactive use is made possible by the Python interactive
interpreter, and its enhanced replacement IPython \citep{perez2007ipython},
which offer a \textsc{matlab}-like working environment specifically designed for
scientific use.

From its core, the library has been designed to tie in with the set of numeric
and scientific packages centered around the NumPy and SciPy libraries. NumPy
\citep{vanderwalt2011} augments Python with a numeric array datatype and
associated array computing primitives while SciPy \citep{varoquaux2013scipy}
extends it further by making common numeric operations available, either by
implementing those in Python/NumPy or by wrapping existing
C/C{}\verb!++!/Fortran implementations. In a similar vein, \sklearn extends
Python with a set of composable machine learning operations.

Started in 2009, \sklearn is developed by an international team of a dozen of
core developers, mostly researchers from various fields (e.g., machine learning,
natural language processing, or neuroscience). The project also greatly benefits
from many occasional contributors proposing small code or documentation fixes or
improvements. Development proceeds on GitHub\footnote{\url{https://github.com/scikit-learn}},
a platform which greatly facilitates this kind of
collaboration. Because of the large number of developers, emphasis is
put on keeping the project maintainable. In particular, code must follow
specific quality guidelines, such as style consistency and unit-test coverage.
In addition, complete documentation and examples are required for all features.
Finally, additions to the code base must pass code review by at least two
developers not involved in the implementation of the proposed addition.

Within the past few years, the project has proved valuable to an ever increasing
user base. Its popularity can be gaged from various indicators such as hundreds
of citations in scientific publications, successes in various machine learning
challenges (e.g., Kaggle), or significant statistics derived from our
repositories and mailing lists.  At the time of writing, the project is watched
by 1309 people and forked 667 times on GitHub, and the mailing-list receives
more than 300 mails per month. The version control logs for the development
version
% ddaa494c116e3c16bf032003c5cccbed851733d2
also count 183 contributors to the project (after manual deduplication).

In a previous paper \citep{pedregosa2011}, \sklearn was briefly presented and
benchmarked against several competitors. In this paper, we instead focus on an
in-depth discussion and analysis of the design choices used in
building the library. First, in section \ref{sec:api}, we present the
central application programming interface (API). Then, in section
\ref{sec:implementation}, we describe the implementation trade-offs.
In section \ref{sec:weaknesses}, we analyze the limitations of the
technologies we rely on; and finally, in section \ref{sec:comparison}, we
compare \emph{scikit-learn} to other major projects in terms of API.

\section{API}

\label{sec:api}

In this section, we discuss the general principles of the application
programming interface (API) of \sklearn. We first specify the
representation of data and then proceed by describing the interface shared by
all objects of the library. We then discuss the composition and reusability
advantages of that interface.

The API is designed to adhere to the following broad principles:
\begin{description}
  \item[Inspection.]
       Constructor parameters
       and parameter values determined by learning algorithms
       are stored as public attributes. All of them are documented in a consistent manner.
       % Gilles: Do we really need to bother the reader with such details?
       % (The Python language does not actually distinguish
       % between public and private attributes of objects,
       % but by convention, any identifier
       % prefixed with an underscore (\texttt{\_})
       % is considered ``a non-public part of the API''; \citealp{pythontut}.)
  \item[Non-proliferation of classes.]
       Learning algorithms are objects of custom classes,
       but datasets and hyperparameter names and values
       are represented as standard Python, NumPy or SciPy classes
       rather than new ones whenever feasible.
       % Gilles: I don't understand the phrase below
       % Custom classes adhere to Python conventions and protocols
       % for constructs such as iteration over the folds
       % in a $k$-fold cross validation.
       This should keep \sklearn easy to use
       and easy to combine with other libraries.
       % XXX do we need to elaborate more on this, or do we expect everyone
       % to be an experienced Java/adapter pattern hater?
  \item[Composition.]
       Many machine learning tasks are expressible
       as sequences or combinations of transformations to data.
       Some learning algorithms are also naturally viewed
       as meta-algorithms parameterized on other algorithms.
       To be able to implement and use these algorithms in a simple way,
       all learners are usable with a consistent set of methods
       and a simple syntax is implemented
       for setting and retrieving parameters of sub-estimators.
       % XXX need to describe this!
  \item[Sensible defaults.]
       Whenever an operation requires a user-defined parameter,
       a default and appropriate value is defined by the library.
       The default value should cause the operation to be performed
       in a sensible way (giving a baseline solution for the task at hand).
\end{description}

\subsection{Data representation}

In most machine learning tasks, data is modeled as a set of variables.  For
example, in a classification or regression task, the goal is to find a mapping
between input variables $X_1, ..., X_p$, called features, and
some output variable $Y$. A sample is then defined as a tuple of values $([x_1,
..., x_p]^\mathrm{T}, y)$ of these variables and a dataset as a collection of
such samples.  As such, matrices of numerical values constitute a natural and
widely used representation in machine learning. In this view, each row in the
matrix then corresponds to a sample of the dataset and each column to one of the
variables of the problem.

In \sklearn, we chose a representation of data that is as close as
possible to the matrix representation: datasets are encoded as NumPy
multidimensional arrays for dense data and as SciPy sparse matrices for sparse
data. While those may seem rather unsophisticated data representations when
compared to more object-oriented constructs, such as those used by
Weka \citep{hall2009weka}, they bring the prime advantage of allowing us to rely
% Lars: "orders of magnitude faster" is an implementation detail
on all NumPy and SciPy vector operations while allowing us at the same time to
keep the code short and readable.  This design choice has also been motivated by
the fact that, given their pervasiveness in many other scientific Python
packages, many scientific users of Python are already familiar with NumPy dense
arrays and SciPy sparse matrices, hence helping them to familiarize with the
project.  From a practical point of view, these formats also provide a bunch of
data loading and conversion tools which make them very easy to use in many
contexts. Moreover, for tasks where the inputs are text files or semi-structured
objects, we provide \textit{vectorizer} objects that efficiently convert such
data to the NumPy or SciPy formats.

The public API is oriented towards batch processing. While classification and
regression algorithms can indeed make predictions for single samples, most \sklearn
objects are however intended to be used on batches of data. For efficiency reasons,
this indeed makes the use of NumPy and SciPy optimal by preventing the overhead
inherent to Python function calls and per-element dynamic type checking.
% Gilles: I don't think it is worth elaborating too much (if at all) on online learning. They are exceptions in scikit-learn.

% The public API is oriented toward batch processing
% to enable efficient implementation.
% While classification and regression algorithms
% can make predictions for single samples,
% and some algorithms in \sklearn can perform online learning
% (e.g. stochastic gradient descent for linear SVMs and logistic regression,
% an online variant of $k$-means based on \citealt{sculley2010web}),
% these algorithm are really intended to be used on small batches of samples.
% Doing so makes optimal use of NumPy and SciPy
% because they prevent the overhead inherent in Python function calls
% and the language's per-element dynamic type checking.


\subsection{Estimators, predictors and transformers}

All objects within \sklearn share a uniform common basic API
consisting of three complementary interfaces: an \textit{estimator} interface
for building and fitting objects, a \textit{predictor} interface for making
predictions and a \textit{transformer} interface for converting data.
As much as possible, our design choices have been guided so as to avoid
the proliferation of framework code. We try to adopt simple conventions and
to limit to a minimum the number of methods an object must implement.

\textbf{Estimators.} The \textit{estimator} interface is at the core of the
library. It defines instantiation mechanisms of objects and exposes a
\texttt{fit} method for learning a model from training data.  All supervised and
unsupervised learning algorithms (e.g., for classification, regression or
clustering) are offered as objects implementing that interface. Machine
learning tasks like feature extraction, feature selection or dimensionality
reduction are also provided as estimators.

The constructor of an estimator object takes as input a set of named constant
hyper-parameters. At initialization, all given parameters are attached to the
estimator instance as public attributes and serve to determine the estimator
behavior (e.g., the $C$ constant in SVMs). Parameters set as public attributes
are also used for introspection purposes, e.g., in model selection routines. To
ease their use, default hyper-parameter values are provided for all built-in
estimators. These default values are set to be relevant in most common
situations in order to make estimators as efficient as possible
\textit{out-of-box} for non-experts. Finally, the constructor of an estimator
never performs any actual learning (this is the role of the \texttt{fit} method,
see below).  Its only goal is to set the parameter values and freeze the
behavior of the estimator.

The estimator interface defines a \texttt{fit} method. This method is called
with training data (e.g., supplied as two arrays \texttt{X\_train} and
\texttt{y\_train} in supervised learning estimators). Its task is to run a
learning algorithm and to determine model- specific parameters from the training
data and set these as attributes on the estimator object. As a convention, the
parameters learned by an estimator are exposed as public attributes whose name
is suffixed with a trailing underscore (e.g., \texttt{coef\_} for the
learned coefficients of a linear model).

As an example, let us consider a supervised learning task using logistic regression.
Given the API defined above, solving this problem is as simple as:
\begin{verbatim}
    from sklearn.linear_model import LogisticRegression
    clf = LogisticRegression(penalty="l1")
    clf.fit(X_train, y_train)
\end{verbatim}
In this snippet, a \texttt{LogisticRegression} estimator is first initialized by
setting the \texttt{penalty} hyper-parameter to \texttt{"l1"}, hence enabling
$\ell_1$ regularization. Other hyper-parameters (such as \texttt{C}, which
defines the strength of the regularization) are not explicitly defined and are
thus set to the default values. Upon calling \texttt{fit}, the model is then
learned from the training arrays \texttt{X\_train} and \texttt{y\_train}. Since
all estimators share the same interface, using a different learning algorithm is
as simple as replacing the constructor. For example, to build a Random Forest on
the same training data, one would simply have to replace
\texttt{LogisticRegression(penalty="l1")} in the snippet above by \\
\texttt{RandomForestClassifier()}.

In \sklearn, classical learning algorithms are not the only objects
to be implemented as estimators. For example, preprocessing routines (e.g.,
vectorization of text documents, scaling of features, etc) or feature
extraction techniques also implement the same interface. Even those estimators
that are entirely stateless and therefore do not require the \texttt{fit} method
to perform any useful work implement the estimator interface. As we will
illustrate in the next sections, this design pattern is indeed of prime
importance for consistency, composition and parameter validation reasons.

\textbf{Predictors.} The \textit{predictor} interface defines a \texttt{predict}
method taking as input some new data \texttt{X\_test} and producing as output
predictions for \texttt{X\_test}, based on the learned parameters of the
estimator (we call the input to \texttt{predict} ``\texttt{X\_test}'' in order
to emphasize that \texttt{predict} generalizes to new data). In the case of
supervised learning estimators, this method typically returns the predicted
labels or values computed by the model.  Continuing with the previous example,
predicted labels for \texttt{X\_test} can be obtained using the following
snippet:
\begin{verbatim}
    y_pred = clf.predict(X_test)
\end{verbatim}
Some unsupervised learning
estimators may however also implement that interface, as illustrated below:
\begin{verbatim}
    from sklearn.cluster import KMeans
    km = KMeans(n_clusters=10)
    km.fit(X_train)
    clust_pred = km.predict(X_test)
\end{verbatim}
In the above example, a \texttt{KMeans} estimator is first fit on the training
data \texttt{X\_train}. Then the \texttt{predict} method is called on the test
data \texttt{X\_test} in order to retrieve predicted cluster labels.
The \texttt{predict} method in \sklearn thus offers a unified way
to compute predictions with respect to unseen data.

Along with the \texttt{predict} method, predictors may also implement methods
for quantifying the certainty of a prediction. In the case of estimators
implementing linear models, the \texttt{decision\_function} method can be used
to measure the distance of the samples from the discriminating hyperplane. Some
predictors also provide a \texttt{predict\_proba} method which returns
class probabilities for the input samples \texttt{X\_test}.

Finally, predictors should also provide a \texttt{score} function to assess the
performance on some input data. In supervised learning estimators, this method
takes as inputs a dataset \texttt{X\_test} and \texttt{y\_test} and typically
computes the coefficient of determination between \texttt{y\_test} and
\texttt{predict(X\_test)} in the case of classification or the mean squared
error in the case of regression. Not however that, from an API point of view,
the only guideline is that the \texttt{score} method returns a numerical value
such that the higher the better. As such, some unsupervised learning estimators
also exposes a \texttt{score} function, computing for instance the likelihood of
the given data.

\textbf{Transformers.} Since it is common to modify or filter data before
feeding it to a learning algorithm, some estimators in the library implement a
\textit{transformer} interface which defines a \texttt{transform} method. It
takes as input some new data \texttt{X\_test} and yields as output a transformed
version of \texttt{X\_test}.
Preprocessing, feature selection and dimensionality
reduction algorithms are all provided as transformers within the library.  In
our example, if one wanted to preprocess the inputs \texttt{X\_train} before
fitting the logistic regression estimator, e.g., by standardizing the feature
values, one could thus simply write:
\begin{verbatim}
    from sklearn.preprocessing import StandardScaler
    scaler = StandardScaler()
    scaler.fit(X_train)
    X_train = scaler.transform(X_train)
\end{verbatim}
Of course, in practice, it is important to apply the same preprocessing to the
test data \texttt{X\_test}. This can easily be done as follows:
\begin{verbatim}
    X_test = scaler.transform(X_test)
\end{verbatim}
Transformers also include a variety of learning algorithms, such as
dimension reduction (PCA, manifold learning techniques), kernel
approximation techniques, and any other approach that can be used to
derive a modified representation of the data.

\subsection{Method chaining}

In \sklearn, the \texttt{fit} method always returns the estimator
it was called on, hence making method chaining possible.  Using this technique,
we can thus rewrite the \texttt{StandardScaler} example above in a single line:
\begin{verbatim}
    X_train = StandardScaler().fit(X_train).transform(X_train)
\end{verbatim}
In addition, \texttt{fit(X\_train).transform(X\_train)} can further
be rewritten as \texttt{fit\_transform(X\_train)}.  Combining the \texttt{fit}
and \texttt{transform} methods into a single method \texttt{fit\_transform}
allows to share computations, which often makes a huge difference in practice.
In the same spirit, \sklearn sometimes provides the
\texttt{fit\_predict} method for combining the \texttt{fit} and \texttt{predict}
methods. This is especially useful for retrieving cluster predictions with
respect to the training data \texttt{X\_train} in clustering algorithms.

\subsection{Estimator composition}

A distinguishing feature of the \sklearn API is its ability to
compose new estimators from several base estimators. Composition mechanisms can
be used to combine typical machine learning workflows into a single object which
is itself an estimator. Thanks to duck typing, composed
estimators can be used wherever usual estimators can be used. In particular,
they integrate within model selection routines, allowing one to optimize at once
every step of a complex workflow. Composition of estimators can be done in two
ways: either sequentially through \texttt{Pipeline} objects or in a parallel
fashion through \texttt{FeatureUnion} objects.

\texttt{Pipeline} objects are used for chaining multiple estimators into a
single one. This is useful since a machine learning workflow typically involves
a fixed sequence of steps in processing data (e.g., feature extraction,
normalization, feature selection, learning and making predictions). A sequence
of $N$ such steps can be combined into a composite object called a
\texttt{Pipeline}. The first $N-1$ steps must be transformers and the last can
be either a predictor or a transformer. Conceptually, fitting a pipeline to a
training set amounts to the following recursive procedure: i) when only one step
remains, call its \texttt{fit} method; ii) otherwise, fit the first step, use it
to \texttt{transform} the training set and fit the rest of the pipeline with the
transformed data.
The pipeline exposes all the methods the last
estimator in the pipe exposes. That is, if the last estimator is a predictor,
the pipeline can itself be used as a predictor. If the last estimator is a
transformer, then the pipeline is itself a transformer. \textcolor{red}{TODO:
Talk about nested attribute syntax.}

A \texttt{FeatureUnion} object is a transformer combining multiple transformers
into a single one that joins their outputs. From an API point of view, a
\texttt{FeatureUnion} takes as input a list of transformers. Calling
\texttt{fit} on the union is the same as calling \texttt{fit} independently on
each of the transformers. For transforming data, the base transformers are
applied in parallel and their outputs are concatenated end-to-end into a larger
output.

As an example, the following snippet illustrates how to create a complex worflow
computing both linear PCA and kernel PCA components on \texttt{X\_train}
(through a \texttt{FeatureUnion}), selecting the 100 best of those features and
then feeding them to an $\ell_2$-regularized logistic regression model:
\begin{verbatim}
    from sklearn.pipeline import Pipeline
    from sklearn.decomposition import PCA, KernelPCA
    from sklearn.feature_selection import SelectKBest

    Pipeline([("pca", FeatureUnion([('pca', PCA()),
                                    ('kpca', KernelPCA(kernel="rbf"))])),
              ("feature_selection", SelectKBest(k=100)),
              ("logistic_model", LogisticRegression(penalty="l2"))
    ]).fit(X_train, y_train).predict(X_test)
\end{verbatim}

As the above snippet shows, \texttt{Pipeline} and \texttt{FeatureUnion} can be
combined to create complex objects. In particular, since both are themselves
estimators, they can be used to form nested workflows. \textcolor{red}{TODO:
elaborate a bit more on nested constructions.}

\subsection{Model selection}

\textcolor{red}{TODO: rewrite and briefly describe the API (see user guide)}\\
\textcolor{red}{TODO: describe Scorer API?}\\
\textcolor{red}{TODO: mention that GridSearchCV objects are themselves estimators}

The aforementioned \texttt{Pipeline} object
can be passed as an argument to a model selection algorithm.
Available algorithms at the time of writing include random search
\citep{bergstra2012} and ``grid search''.
The latter, which we shall use in this example,
performs an exhaustive search through a grid formed by the cartesian product
of a set of possible values for each parameter, specified by the user.
In our example, this grid might be given as:

\begin{align*}
         & \textsf{stopwords} \in \{0, 1\}                      \\
  \times & \; \textsf{tf} \in \{\textsf{linear}, \textsf{log}\} \\
  \times & \; n_\textsf{features} \in \{1000, 2000, 5000\}      \\
  \times & \; \textsf{norm} \in \{\ell_1, \ell_2\}              \\
  \times & \; C \in \{10, 100, 1000\}
\end{align*}

At each point in this grid, $k$-fold cross validation is run
to estimate the performance of the estimator according to some measure
(e.g., accuracy, $F_\beta$-score or a host of clustering quality metrics)
and the best performing set of hyperparameters is stored
on the \texttt{GridSearchCV} object, again, as a public attribute.

\section{Implementation}

\label{sec:implementation}

\noindent \textcolor{red}{TODO: Mention small set of dependencies.}\\
\textcolor{red}{TODO: elaborate on use of mixins and duck typing}

scikit-learn is primarily implemented in Python and Cython
\citep{behnel2011cython},
a language that extends Python with static typing
and a compiler that produces C extension modules
for the Python runtime system.
In addition, it includes (modified versions of)
the \textsf{LIBSVM} and \textsf{LIBLINEAR} libraries
used for training support vector machines
and logistic regression models \citep{chang2011libsvm, fan2008}.
These libraries are both written in C{}\verb!++!
and wrapped using Cython modules.

As much as possible is implemented in ``pure'' Python,
to avoid the compile-edit-debug cycle needed for Cython
and to keep the codebase readable for as large an audience as possible.
However, since the speed of the current Python implementation
is not sufficient for numeric programming
and NumPy's vector operations are not appropriate for all use cases
(among other factors, because they often have linear space requirements),
some of the core algorithms have to be implemented in Cython.\footnote{
  The only Python implementation currently supported by scikit-learn
  is the reference implementation, CPython.
  The alternative Python implementation PyPy \citep{bolz2009tracing}
  promises to alleviate this problem by JIT compilation,
  but does not yet interoperate with NumPy and SciPy.
}
Examples include the aforementioned $k$-means implementation,
stochastic gradient descent for linear models
and some graph-based clustering algorithms.

While object-oriented techniques and design patterns
are used throughout scikit-learn,
they mostly serve to facilitate code reuse and are not considered a project goal.
Where possible, the Python principle of ``duck typing'' is exploited
to simplify the implementation and skip the introduction of superfluous classes.
This allows for extensibility and flexibility at the same time:
user-defined estimators that follow scikit-learn conventions
should be usable in pipelines and other composite objects
without them actually inheriting from scikit-learn base classes.

\section{Weaknesses of the NumPy/SciPy environment}

\label{sec:weaknesses}

\textcolor{red}{TODO: move this into the Implementation section? These are mostly implementation considerations.}

% This needs an introductory paragraph, but the best I could come up with was:
As can be expected, the NumPy/SciPy environment is not completely flawless.

% The following hedge needs to be in here; we don't want to diss our peers
It should be noted that this section is meant
as a summary of challenges faced by scikit-learn development
and an attempt at constructive criticism.
There is considerable overlap between the communities
that develop or contribute to NumPy, SciPy, scikit-learn, and Cython,
so the issues raised here may eventually be resolved
by the scikit-learn developers themselves.

\subsection{Sparse matrix support}

SciPy's handling of sparse matrices is one source of confusion
for scikit-learn developers and users alike.
The \texttt{scipy.sparse} package offers support
for this kind of data structure,
which is crucial to achieving performance and scalability
in the face of high-dimensional inputs
(including, but not limited to, NLP applications).
However, the interface exposed by its classes
deviates from the NumPy array interface, sometimes subtly so,
sometimes entirely.  % XXX is this grammatically correct?
For example, while NumPy arrays have a variable number of dimensions,
so that vectors can be considered 1-d arrays
(without a distinction between row and column vectors),
matrices are 2-d, etc., \texttt{scipy.sparse} matrices are always 2-dimensional,
meaning that special care has to be taken when handling sparse vectors.

The development team has built up sufficient familiarity
with the sparse/array distinction to factor out common patterns
for supporting both sparse and dense input data structures in many estimators.
In other cases, though, sparse matrix support has to implemented
on a per-estimator basis.

\subsection{Parallelism}

At this point in time, support for multi-core programming in Python,
at least in the CPython reference implementation targeted by scikit-learn,
is downright weak.
The interpreter supports multithreading, but due to the ``Global Interpreter Lock'' (GIL),
no two threads in a process may at the same time execute Python code.
It follows that multithreading is not a viable solution
even two embarrassingly parallel problems,
such as fitting and evaluation multiple models in a grid search procedure.
The workaround is to use Python's multiprocessing module,
which emulates threads using multiple operating system-level processes,
but this in turn leads to high memory use and overhead
because of the copying of training and test sets
into the various processes' address spaces.

While Cython makes writing performance-critical code possible
in a language that is very much like Python,
doing so properly still requires some knowledge of C.
One source of trouble has been the various fixed-size integer types
that are used by the Python and NumPy C runtimes,
whose sizes are practically all platform-dependent.

Another problem concerns the mismatch between on the one hand Python functions,
which are first-class runtime objects endowed with methods and metadata
that identify them as ``callable'',
that dynamically check their number and types of arguments,
and that can return any Python type of object,
and compiled C or Cython functions on the other hand,
which exhibit static typing.
Where it may sometimes be convenient for estimators
to take functions as arguments
(e.g., in the case of kernel machine learning,
where one might like to use a custom kernel),
but such usage is not at present possible and may never be.
Passing C or Cython-compiled functions would require an API for the kernel method
to be implemented at the C or Cython level,
only to be used from compiled code.
Passing arbitrary Python functions would be possible,
but performance can be expected to degrade significantly
because of the runtime checks needed for each invocation of the kernel function.
(The current solution in scikit-learn is to let the user pass not a kernel function,
but a precomputed Gram matrix for an entire set of objects.
The linear, polynomial and RBF kernels are treated as special cases in
the \textsf{LIBSVM}-based SVM learner, where they are implemented in C{}\verb!++!.)

\section{Comparison with other packages}

\label{sec:comparison}

\textcolor{red}{TODO: compare with weka, R, vowpal rabbit, ... ?}\\
\textcolor{red}{TODO: emphasize scikit-learn's place in the overall scipy ecosystem, and its role in spurring the development of more basic tools.}\\
\textcolor{red}{TODO: some packages start to adopt our own API.}

Recent years have witnessed a rising interest in machine learning and data mining
with applications in many fields.
With this rise comes a host of machine learning packages
(both open source and proprietary) with which \sklearn competes.

% XXX Is the Julia comparison fair?
% The goal of that language seems to be a bit different:
% to provide a compiler to implement numeric algorithms without vectorized idioms,
% maybe more like Cython than Python...
In comparison to specialized languages for numeric and statistical programming
such as \textsc{matlab}, R \citep{trancon2012r} and Julia \citep{bezanson2012julia},
the use of Python as a host language has the distinct advantage
that it is a mature, \textit{general purpose} language:
it has strong language and standard library support for such tasks as
string handling/text processing, interprocess communication, networking
and many of the other tasks that many machine learning programs
(whether academic or commercial)
will have to perform ``on the side''.
In addition, third-party libraries are available for virtually any task
that modern computer programs routinely perform.

\section{Future work}

ToDo.

\bibliographystyle{plainnat}
\DeclareRobustCommand{\VAN}[3]{#3}
\bibliography{paper}

\end{document}
