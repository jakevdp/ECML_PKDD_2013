\documentclass{llncs}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
%\usepackage{hyperref}
\usepackage[authoryear,round]{natbib}
\usepackage{xcolor}
\usepackage{xspace}
\usepackage{minted}

\definecolor{rulecolor}{rgb}{0.80,0.80,0.80}
\newminted{python}{frame=single,rulecolor=\color{rulecolor}}

% tt font with bold support
%\renewcommand{\ttdefault}{pcr}

\pagestyle{headings}

\newcommand{\sklearn}{\textit{scikit-learn}\xspace}

% working title, please change
\title{Scikit-learn: API design for machine learning}

\author{Lars~Buitinck~\inst{1,2} \and
        Gilles~Louppe~\inst{1,2} \and
        Mathieu~Blondel~\inst{1,2} \and
        Fabian~Pedregosa~\inst{2} \and
        Andreas~Mueller~\inst{2} \and
        Olivier~Grisel~\inst{2} \and
        Vlad~Niculae~\inst{2} \and
        Peter~Prettenhofer~\inst{2} \and
        Alexandre~Gramfort~\inst{2} \and
        Jacques~Grobler~\inst{2} \and
        Robert~Layton~\inst{2} \and
        Jake~Vanderplas~\inst{2} \and
        Arnaud~Joly~\inst{2} \and
        Brian Holt~\inst{2} \and
        GaÃ«l~Varoquaux~\inst{2}}

\institute{Main authors of the present paper \and
           Core contributors to the project}

% Dutch name sorting hack as per http://tex.stackexchange.com/a/40750/2806
\DeclareRobustCommand{\VAN}[3]{#2}

\begin{document}

\maketitle

\begin{abstract}
The \sklearn project is an increasingly popular open source machine learning
library written in Python and designed to be simple and efficient, accessible to
non-experts, and reusable in various contexts. In this paper, we present and
discuss our design choices for the application programming interface (API) of
the project. In particular, we describe the simple and elegant interface shared
by all learning and processing units in the library and then discuss its
advantages in terms of composition and reusability. The paper also comments on
implementation details specific to the Python ecosystem and analyzes obstacles
faced by users and developers of the library.
\end{abstract}

\section{The scikit-learn project}

The \sklearn project \citep{pedregosa2011} is an open source machine learning
library for the Python programming language. The ambition of the project is to
provide efficient and well-established machine learning tools within a
programming environment that is accessible to non-machine learning experts and
reusable in various scientific areas. The project is not a novel domain-specific
language, but a library that provides machine learning idioms to a general-
purpose high-level language. Among other things, it includes classical learning
algorithms, model selection tools and preprocessing procedures. The project is
distributed under the simplified BSD license, encouraging its use in both
academic and commercial settings.

In itself, the project is a programming library, i.e. a collection of classes
and functions that users import into Python programs. Using \sklearn therefore
requires basic Python programming knowledge. No command-line interface, let
alone a graphical user interface, is offered for non-programmer users. Instead,
interactive use is made possible by the Python interactive interpreter, and its
enhanced replacement IPython \citep{perez2007ipython}, which offer a
\textsc{matlab}-like working environment specifically designed for scientific
use.

From its core, the library has been designed to tie in with the set of numeric
and scientific packages centered around the NumPy and SciPy libraries. NumPy
\citep{vanderwalt2011} augments Python with a numeric array data type and proper
array computing primitives while SciPy \citep{varoquaux2013scipy} extends it
further by making common numerical operations available, either by implementing
those in Python/NumPy or by wrapping existing C/C{}\verb!++!/Fortran
implementations. In a similar vein, \sklearn extends Python with a set of
composable machine learning operations.

Started in 2007, \sklearn is developed by an international team of a dozen of
core developers, mostly researchers from various fields (e.g., machine learning,
natural language processing, or neuroscience). The project also greatly benefits
from many occasional contributors proposing small code or documentation fixes or
improvements. Development proceeds on GitHub\footnote{\url{https://github.com/scikit-learn}},
a platform which greatly facilitates this kind of
collaboration. Because of the large number of developers, emphasis is
put on keeping the project maintainable. In particular, code must follow
specific quality guidelines, such as style consistency and unit-test coverage.
In addition, complete documentation and examples are required for all features.
Finally, additions to the code base must pass code review by at least two
developers not involved in the implementation of the proposed addition.

Within the past few years, the project has proved valuable to an ever increasing
user base. Its popularity can be gaged from various indicators such as hundreds
of citations in scientific publications, successes in various machine learning
challenges (e.g., Kaggle), or significant statistics derived from our
repositories and mailing lists.  At the time of writing, the project is watched
by 1309 people and forked 667 times on GitHub, and the mailing-list receives
more than 300 mails per month. The version control logs for the development
version
% ddaa494c116e3c16bf032003c5cccbed851733d2
also count 183 unique contributors to the project.

In a previous paper \citep{pedregosa2011}, \sklearn was briefly presented and
benchmarked against several competitors. In this paper, we instead focus on an
in-depth discussion and analysis of the design choices used in building the
library. We first present in section \ref{sec:core-api} the central application
programming interface (API) and then describe, in section \ref{sec:advanced-api},
advanced API mechanisms built from the core interface. Section
\ref{sec:implementation} describes implementation principles and discusses
limitations of the technologies we rely on. Section \ref{sec:comparison} then
compares \emph{scikit-learn} to other major projects in terms of API. Finally,
conclusions and directions of future works are included in section
\ref{sec:conclusions}.

\section{Core API}

\label{sec:core-api}

All objects within \sklearn share a uniform common basic API consisting of three
complementary interfaces: an \textit{estimator} interface for building and
fitting models, a \textit{predictor} interface for making predictions and a
\textit{transformer} interface for converting data. As much as possible, our
design choices have been guided so as to avoid the proliferation of framework
code. We try to adopt simple conventions and to limit to a minimum the number of
methods an object must implement. The API is designed to adhere to the following
broad principles:

\begin{description}
  \item[Consistency.]
       All objects (basic or composed) share a consistent interface composed of
       a limited set of methods. This interface is documented in a consistent
       manner for all objects.
  \item[Inspection.]
       Constructor parameters and parameter values determined by learning
       algorithms are stored and exposed as public attributes.
  \item[Non-proliferation of classes.]
       Learning algorithms are the only objects to be of custom classes.
       Datasets and hyper-parameter names and values are represented as standard
       Python, NumPy or SciPy classes rather than new ones whenever feasible.
       This keeps \sklearn easy to use and easy to combine with other libraries.
       % XXX do we need to elaborate more on this, or do we expect everyone
       % to be an experienced Java/adapter pattern hater?
  \item[Composition.]
       Many machine learning tasks are expressible
       as sequences or combinations of transformations to data.
       Some learning algorithms are also naturally viewed
       as meta-algorithms parametrized on other algorithms.
       Whenever possible, those algorithms are implemented and composed from
       existing building blocks.
  \item[Sensible defaults.]
       Whenever an operation requires a user-defined parameter,
       a default and appropriate value is defined by the library.
       The default value should cause the operation to be performed
       in a sensible way (giving a baseline solution for the task at hand).
\end{description}

\subsection{Data representation}
\label{sec:arrays}

In most machine learning tasks, data is modeled as a set of variables.  For
example, in a classification or regression task, the goal is to find a mapping
between input variables $X_1, ..., X_p$, called features, and
some output variable $Y$. A sample is then defined as a tuple of values $([x_1,
..., x_p]^\mathrm{T}, y)$ of these variables and a dataset as a collection of
such samples.  As such, matrices of numerical values constitute a natural and
widely used representation in machine learning. In this view, each row in the
matrix then corresponds to a sample of the dataset and each column to one of the
variables of the problem.

In \sklearn, we chose a representation of data that is as close as
possible to the matrix representation: datasets are encoded as NumPy
multidimensional arrays for dense data and as SciPy sparse matrices for sparse
data. While those may seem rather unsophisticated data representations when
compared to more object-oriented constructs, such as those used by
Weka \citep{hall2009weka}, they bring the prime advantage of allowing us to rely
% Lars: "orders of magnitude faster" is an implementation detail
on all NumPy and SciPy vector operations while allowing us at the same time to
keep the code short and readable.  This design choice has also been motivated by
the fact that, given their pervasiveness in many other scientific Python
packages, many scientific users of Python are already familiar with NumPy dense
arrays and SciPy sparse matrices.
% Gilles: the phrase was too long
% hence helping them to familiarize with the project.
From a practical point of view, these formats also provide a bunch of
data loading and conversion tools which make them very easy to use in many
contexts. Moreover, for tasks where the inputs are text files or semi-structured
objects, we provide \textit{vectorizer} objects that efficiently convert such
data to the NumPy or SciPy formats.

For efficiency reasons, the public interface is oriented towards processing
batches of samples rather than single samples per API call. While classification
and regression algorithms can indeed make predictions for single samples,
\sklearn objects are not optimized for this use case. (The few online learning
algorithms implemented are intended to take mini-batches.) Batch processing makes
optimal use of NumPy and SciPy by preventing the overhead inherent to Python
function calls or due to per-element dynamic type checking. Although this might
seem to be an artifact of the Python language, and therefore an implementation
detail that leaks into the API, we argue that APIs should be designed so as not
to tie a library to a suboptimal implementation strategy. As such, batch
processing enable fast implementations in lower-level languages (where memory
hierarchy effects and the possibility of internal parallelization come into
play).


\subsection{Estimators}
\label{sec:estimators}

The \textit{estimator} interface is at the core of the
library. It defines instantiation mechanisms of objects and exposes a
\texttt{fit} method for learning a model from training data.  All supervised and
unsupervised learning algorithms (e.g., for classification, regression or
clustering) are offered as objects implementing that interface. Machine
learning tasks like feature extraction, feature selection or dimensionality
reduction are also provided as estimators.

Estimator initialization and actual learning are strictly separated,
in a way that is similar to partial function application:
an estimator is initialized from a set of named constant hyper-parameter values
(e.g., the $C$ constant in SVMs)
and can be considered as a function
that maps these values to actual learning algorithms.
The constructor of an estimator does not see any actual data, nor does it perform any actual learning.
All it does is attach the given parameters to the object.
For the sake of convenient model inspection, hyper-parameters are set as public attributes,
which is especially important in model selection settings.
For ease of use, default hyper-parameter values are also provided
for all built-in estimators.
These default values are set to be relevant in most common
situations in order to make estimators as effective as possible
\textit{out-of-box} for non-experts.

Actual learning is performed by the \texttt{fit} method. This method is called
with training data (e.g., supplied as two arrays \texttt{X\_train} and
\texttt{y\_train} in supervised learning estimators). Its task is to run a
learning algorithm and to determine model-specific parameters from the training
data and set these as attributes on the estimator object. As a convention, the
parameters learned by an estimator are exposed as public attributes whose name
is suffixed with a trailing underscore (e.g., \texttt{coef\_} for the
learned coefficients of a linear model),
again to facilitate model inspection.
In the partial application view,
\texttt{fit} is a function from data to a model of that data.
As such, it always returns the estimator object it was called on,
which now serves as a model of its input and can be used to perform predictions or transformations of input data.
The choice to let a single object serve dual purpose as estimator and model
simplifies the \sklearn class hierarchy
compared to an approach were estimators and models are separated.

As an example to illustrate the initialize-\texttt{fit} sequence,
let us consider a supervised learning task using logistic regression.
Given the API defined above, solving this problem is as simple as:
\begin{pythoncode}
from sklearn.linear_model import LogisticRegression

clf = LogisticRegression(penalty="l1")
clf.fit(X_train, y_train)
\end{pythoncode}
In this snippet, a \texttt{LogisticRegression} estimator is first initialized by
setting the \texttt{penalty} hyper-parameter to \texttt{"l1"}, hence enabling
$\ell_1$ regularization. Other hyper-parameters (such as \texttt{C}, which
defines the strength of the regularization) are not explicitly defined and are
thus set to the default values. Upon calling \texttt{fit}, the model is then
learned from the training arrays \texttt{X\_train} and \texttt{y\_train}.
(In this example, \texttt{fit} is called solely for its side effects.)
Since all estimators share the same interface, using a different learning algorithm is
as simple as replacing the constructor (the class name).
For example, to build a random forest on
the same training data, one would simply have to replace
\texttt{LogisticRegression(penalty="l1")} in the snippet above by \\
\texttt{RandomForestClassifier()}.

In \sklearn, classical learning algorithms are not the only objects to be
implemented as estimators. For example, preprocessing routines (e.g., scaling of
features) or feature extraction techniques (e.g., vectorization of text
documents) also implement the same interface. Even those estimators that are
entirely stateless and therefore do not require the \texttt{fit} method to
perform any useful work implement the estimator interface. As we will illustrate
in the next sections, this design pattern is indeed of prime importance for
consistency, composition and model selection reasons.

\subsection{Predictors}

The \textit{predictor} interface extends the notion of an estimator
by adding a \texttt{predict}
method that takes an array \texttt{X\_test} and produces
predictions for \texttt{X\_test}, based on the learned parameters of the
estimator (we call the input to \texttt{predict} ``\texttt{X\_test}'' in order
to emphasize that \texttt{predict} generalizes to new data). In the case of
supervised learning estimators, this method typically returns the predicted
labels or values computed by the model.  Continuing with the previous example,
predicted labels for \texttt{X\_test} can be obtained using the following
snippet:
\begin{pythoncode}
y_pred = clf.predict(X_test)
\end{pythoncode}
Some unsupervised learning
estimators may however also implement that interface, as illustrated below:
\begin{pythoncode}
from sklearn.cluster import KMeans

km = KMeans(n_clusters=10)
km.fit(X_train)
clust_pred = km.predict(X_test)
\end{pythoncode}
In the above example, a \texttt{KMeans} estimator is first fit on the training
data \texttt{X\_train}. Then the \texttt{predict} method is called on the test
data \texttt{X\_test} in order to retrieve predicted cluster labels.
The \texttt{predict} method in \sklearn thus offers a unified way
to compute predictions with respect to unseen data.

Along with the \texttt{predict} method, predictors may also implement methods
for quantifying the certainty of a prediction. In the case of estimators
implementing linear models, the \texttt{decision\_function} method can be used
to measure the distance of the samples from the discriminating hyperplane. Some
predictors also provide a \texttt{predict\_proba} method which returns
class probabilities for the input samples \texttt{X\_test}.

Finally, predictors should also provide a \texttt{score} function to assess the
performance on some input data. In supervised learning estimators, this method
takes as inputs arrays \texttt{X\_test} and \texttt{y\_test} and typically
computes the coefficient of determination between \texttt{y\_test} and
\texttt{predict(X\_test)} in the case of classification, or the mean squared
error in the case of regression. Not however that, from an API point of view,
the only guideline is that the \texttt{score} method returns a value
that quantifies the quality of its predictions (the higher, the better).
Unsupervised learning estimators may also exposes a \texttt{score} function
to compute, for instance, the likelihood of the given data under its model.

\subsection{Transformers}

Since it is common to modify or filter data before feeding it to a learning
algorithm, some estimators in the library implement a \textit{transformer}
interface which defines a \texttt{transform} method. It takes as input some new
data \texttt{X\_test} and yields as output a transformed version of
\texttt{X\_test}. Preprocessing, feature selection and dimensionality reduction
algorithms are all provided as transformers within the library.  In our example,
if one wanted to preprocess the inputs \texttt{X\_train} before fitting the
logistic regression estimator, e.g., by standardizing the feature values, one
could thus simply write:
\begin{pythoncode}
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(X_train)
X_train = scaler.transform(X_train)
\end{pythoncode}
Of course, in practice, it is important to apply the same preprocessing to the
test data \texttt{X\_test}. This can easily be done as follows:
\begin{pythoncode}
X_test = scaler.transform(X_test)
\end{pythoncode}
Transformers also include a variety of learning algorithms, such as
dimension reduction (PCA, manifold learning techniques), kernel
approximation techniques, and any other approach that can be used to
derive a modified representation of the data.

Additionally, by leveraging the fact that \texttt{fit} always returns the
estimator it was called on, the \texttt{StandardScaler} example above can be
rewritten in a single line using method chaining:
\begin{pythoncode}
X_train = StandardScaler().fit(X_train).transform(X_train)
\end{pythoncode}
In this particular example, \texttt{fit(X\_train).transform(X\_train)} can also further
be rewritten as \texttt{fit\_transform(X\_train)}.  Combining the \texttt{fit}
and \texttt{transform} methods into a single \texttt{fit\_transform} method
indeed allows to share computations, which often makes a huge difference in practice.
In the same spirit, clustering algorithms provide a
\texttt{fit\_predict} method
that is equivalent to \texttt{fit} followed by \texttt{predict},
and thus returns the cluster labels assigned to the training data samples.


\section{Advanced API}

\label{sec:advanced-api}

Building on the core interface introduced in the previous section, we now
present in this section advanced API mechanisms for building meta-estimators,
composing complex estimators and then selecting models.

\subsection{Meta-estimators}

Some machine learning algorithms are most naturally expressed
as modification or extensions of other, simpler algorithms.
Examples include ensemble estimators
and various multiclass and multilabel classification schemes.
Such algorithms are implemented as so-called ``meta-estimators'',
that take an estimator as input and use that to perform learning and prediction.

As an example, a logistic regression classifier
by default uses a one-vs.-rest (aka one-vs.-all) scheme internally
for performing multiclass classification.
If multiclass classification based on error-correcting codes
\citep{dietterich1995solving} is desired instead,
this can achieved with an \texttt{OutputCodeClassifier}
that wraps a \texttt{LogisticRegression}:
\begin{pythoncode}
from sklearn.linear_model import LogisticRegression
from sklearn.multiclass import OutputCodeClassifier

ecoc_lr = OutputCodeClassifier(LogisticRegression(penalty="l1"))
\end{pythoncode}

% TODO: show a grid search on this estimator

\subsection{Pipelines and feature unions}

A distinguishing feature of the \sklearn API is its ability to
compose new estimators from several base estimators. Composition mechanisms can
be used to combine typical machine learning workflows into a single object which
is itself an estimator. Thanks to duck typing, composed
estimators can be used wherever usual estimators can be used. In particular,
they integrate within model selection routines, allowing one to optimize at once
every step of a complex workflow. Composition of estimators can be done in two
ways: either sequentially through \texttt{Pipeline} objects or in a parallel
fashion through \texttt{FeatureUnion} objects.

\texttt{Pipeline} objects are used for chaining multiple estimators into a
single one. This is useful since a machine learning workflow typically involves
a fixed sequence of steps in processing data (e.g., feature extraction,
normalization, feature selection, learning and making predictions). A sequence
of $N$ such steps can be combined into a composite object called a
\texttt{Pipeline}. The first $N-1$ steps must be transformers and the last can
be either a predictor or a transformer. Conceptually, fitting a pipeline to a
training set amounts to the following recursive procedure: i) when only one step
remains, call its \texttt{fit} method; ii) otherwise, \texttt{fit} the first
step, use it to \texttt{transform} the training set and \texttt{fit} the rest of
the pipeline with the transformed data. The pipeline exposes all the methods the
last estimator in the pipe exposes. That is, if the last estimator is a
predictor, the pipeline can itself be used as a predictor. If the last estimator
is a transformer, then the pipeline is itself a transformer.

A \texttt{FeatureUnion} object is a transformer combining multiple transformers
into a single one that joins their outputs. From an API point of view, a
\texttt{FeatureUnion} takes as input a list of transformers. Calling
\texttt{fit} on the union is the same as calling \texttt{fit} independently on
each of the transformers. For transforming data, the base transformers are
applied in parallel and their outputs are concatenated end-to-end into a larger
output.

As an example, the following snippet illustrates how to create a complex worflow
computing both linear PCA and kernel PCA components on \texttt{X\_train}
(through a \texttt{FeatureUnion}),
selecting the 100 best of those features according to an ANOVA test
and then feeding them to an $\ell_2$-regularized logistic regression model:
\begin{pythoncode}
from sklearn.pipeline import FeatureUnion, Pipeline
from sklearn.decomposition import PCA, KernelPCA
from sklearn.feature_selection import SelectKBest
from sklearn.linear_model import LogisticRegression

fu = FeatureUnion([("pca", PCA()),
                   ("kpca", KernelPCA(kernel="rbf"))])

Pipeline([("feat_union", fu),
          ("feat_sel", SelectKBest(k=100)),
          ("log_reg", LogisticRegression(penalty="l2"))
]).fit(X_train, y_train).predict(X_test)
\end{pythoncode}

As the above example shows, \texttt{Pipeline} and \texttt{FeatureUnion} can be
combined to create complex and nested workflows.

\subsection{Model selection}

As introduced in Section~\ref{sec:estimators}, hyper-parameters set in the
constructor of an estimator determine its behavior. In particular, depending on
the problem, some hyper-parameters produce estimators that are better than
others. The problem of \textit{model selection} is therefore to find, within
some hyper-parameter space, the best combination of hyper-parameters, with
respect to some user-chosen criterion. For example, in the case of a decision
tree, too small values of the \texttt{max\_depth} parameter will make the tree
underfit while too large values will make the tree overfit the training data.
The problem is therefore to find the best trade-off in order to maximize
the performance of the tree on unseen data.

In \sklearn, model selection is supported in two distinct meta-estimators,
\texttt{GridSearchCV} and \texttt{RandomizedSearchCV}.  They take as inputs an
estimator (basic or composite), whose hyper-parameters must be optimized, and a
hyper-parameter grid, which defines the values to search. Optionally, they can
also take a cross-validation scheme and a score function.  The library provides
various such cross-validation schemes, including K-Fold (default), Leave-One-Out
and Bootstrap. The score function used by default is the \texttt{score} method
provided by the estimator. However, the library provides a variety of
score functions that the user can choose instead, including -- among many others
-- accuracy, AUC, $F_1$-score for classification; $R^2$ score or mean squared
error for regression and the Rand Index for clustering.

\texttt{GridSearchCV} and \texttt{RandomizedSearchCV} differ in the way they
search the hyper-parameter grid.  \texttt{GridSearchCV} exhaustively searches
for all hyper-parameter combinations from the grid. This scheme can be expensive
when there are many possible hyper-parameter combinations.
\texttt{RandomizedSearchCV}, on the other hand, only searches for a limited
number of combinations, randomly sampled from the grid. Such scheme has been
shown to work well in practice while considerably reducing the computational
cost \citep{bergstra2012}. For each hyper-parameter combination and each train /
validation split generated by the cross-validation scheme, \texttt{GridSearchCV}
and \texttt{RandomizedSearchCV} fit the base estimator on the training set and
evaluate its performance on the validation set.  In the end, the best performing
model on average is retained and exposed as the public attribute
\texttt{best\_estimator\_}.

As an example, the snippet below illustrates how to optimize the
hyper-parameters of an SVM classifier. It maximizes the $F_1$ score of the
classifier through a 10-fold cross-validation scheme.
\begin{pythoncode}
from sklearn.svm import SVC
from sklearn.grid_search import GridSearchCV

param_grid = [
  {"C": [1, 10, 100, 1000], "kernel": ["linear"]},
  {"C": [1, 10, 100, 1000], "gamma": [0.001, 0.0001],
   "kernel": ["rbf"]},
]

clf = GridSearchCV(SVC(), param_grid, scoring="f1", cv=10)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
\end{pythoncode}
Note that, in the above example, two distinct hyper-parameter grids are
considered. This is because it only makes sense to search for \texttt{gamma}
when \texttt{kernel} is set to \texttt{"rbf"}.  Additionally, we see that
\texttt{GridSearchCV} has a \texttt{predict} method, like any other classifier.
This is because \texttt{GridSearchCV} and \texttt{RandomizedSearchCV} in fact
delegate the \texttt{predict}, \texttt{predict\_proba}, \texttt{transform} and
\texttt{score} methods to the best estimator, thus mimicking its behavior.

\subsection{Extending scikit-learn}

To ease code reuse, simplify implementation and skip the introduction of
superfluous classes, the principle of \textit{duck typing} is exploited all
throughout the codebase. This technique allows both for extensibility and
flexibility: as long as an estimator follows our API and conventions (as
outlined in Section~\ref{sec:core-api}), then it should be as reusable as any
built-in estimator (e.g., it can plugged within pipelines or grid search
estimators). That is, estimators are specified only by their interface, and not
by inheritance. In particular, this means that external developers can define
their own estimators without being forced to inherit from any internal class of
\sklearn, and rest assured that they can be used as first-class estimators.

In other places of the library, in particular in the feature extraction and
vectorization code for text documents, the toolkit is also designed to be
extensible. Here, estimators provide hooks for user-defined code: objects or
functions that provide a specific API can be given as arguments at vectorizer
construction time. The library then calls into this code to obtain the desired
behavior. Again, such external user code can be kept agnostic of the \sklearn
class hierarchy and operates, for example, on standard Python objects such as
strings instead.

Our rule of thumb is that user code should not be tied to \sklearn -- which is a
\textit{library}, and not a \textit{framework}. This principle indeed avoids a
well-known problem with object-oriented design, which is that users wanting a
``banana'' should not get ``a gorilla holding the banana and the entire jungle''
(Joe~Armstrong, interviewed by \citealp{seibel2009coders}). That is, programs
using \sklearn should be easily reusable with other machine learning toolkits or
in other contexts, because they don't need to be intimately tied to it.


\section{Implementation}
\label{sec:implementation}

Our implementation guidelines emphasize on writing efficient but readable code.
In particular, we focus on making the codebase easily maintainable and
understandable in order to favour external contributions. Whenever practicable,
algorithms implemented in \sklearn are thus directly written in pure Python. For
intensive numerical computations though, the library mostly relies on NumPy
vector operations, which allows for the code to remain concise, readable and
efficient. For all algorithms that cannot be easily and efficiently expressed as
NumPy operations, we rely on Cython \citep{behnel2011cython}  for writing the
critical parts and achieving decent performance and scalability. Cython is a
compiled programming language that extends Python with static typing. It
produces efficient C extension modules that are directly importable from the
Python run-time system. Examples of algorithms written in Cython include
stochastic gradient descent for linear models, some graph-based clustering
algorithms or decision trees.  A problem arising from the use of Cython,
however, is that passing pure Python functions as argument, while possible,
degrades performance significantly due to the run-time checks needed for each
function invocation. This is especially problematic for example for kernel
methods, which would benefit from accepting a kernel function as argument. A
potential solution to this problem would be for \sklearn to provide a Cython-level
API, thereby allowing the user to pass Cython-implemented kernel
functions.

To facilitate the adoption and the distribution of the library, we also try to
limit to a bare minimum the set of external dependencies. As such, NumPy and
SciPy -- the two most common dependencies in the Python scientific ecosystem --
are actually the only two required dependencies for the library to fully work.
Some modules may rely on other external libraries, but in such cases we tend to
prefer to directly ship them within the codebase rather than adding new
dependencies for the user to install. In particular, the codebase includes
modified versions of the \textsf{LIBSVM} and \textsf{LIBLINEAR} libraries
\citep{chang2011libsvm,fan2008}, both written in C{}\verb!++! and wrapped using
Cython modules.

\section{Comparison with other packages}

\label{sec:comparison}

\textcolor{red}{TODO: compare with weka, R, vowpal rabbit, ... ?}\\
\textcolor{red}{TODO: emphasize scikit-learn's place in the overall scipy ecosystem, and its role in spurring the development of more basic tools.}\\
\textcolor{red}{TODO: some packages start to adopt our own API.}

Recent years have witnessed a rising interest in machine learning and data mining
with applications in many fields.
With this rise comes a host of machine learning packages
(both open source and proprietary) with which \sklearn competes.

% XXX Is the Julia comparison fair?
% The goal of that language seems to be a bit different:
% to provide a compiler to implement numeric algorithms without vectorized idioms,
% maybe more like Cython than Python...
In comparison to specialized languages for numeric and statistical programming
such as \textsc{matlab}, R \citep{trancon2012r} and Julia \citep{bezanson2012julia},
the use of Python as a host language has the distinct advantage
that it is a mature, \textit{general purpose} language:
it has strong language and standard library support for such tasks as
string handling/text processing, interprocess communication, networking
and many of the other tasks that many machine learning programs
(whether academic or commercial)
will have to perform ``on the side''.
In addition, third-party libraries are available for virtually any task
that modern computer programs routinely perform.

Another package that deserves mention is the Gensim topic modeling toolkit
\citep{rehurek2010gensim}.
Although its stated goal is not machine learning,
it does exemplify a style of API design for scalable processing of ``big data''
that takes a different approach than the comparable models in \sklearn.
Gensim's way of dealing with large datasets is to use algorithms
that have $O(1)$ space complexity and can be updated online,
called \textit{document streaming} by \citeauthor{rehurek2010gensim}.
The API is designed around the Python concept of an \textit{iterable}
(supported in the language by a restricted form of co-routines called
\textit{generators}).
While the text vectorization part of \sklearn
also uses irerables to some extent,
the emphasis is still largely on (mini-)batch processing,
even in the truly stateless, O(1) memory vectorizers
that implement the hashing trick of \citet{weinberger2009}.
This way of processing, as argued earlier in Section~\ref{sec:arrays},
reduces various forms of overhead
and allows users to make use of the abundance of memory in modern machines.
No attempt is made to hide this batch-oriented processing from the user,
allowing control over the amount of memory actually dedicated
to \sklearn algorithms.


\section{Conclusions and future work}
\label{sec:conclusions}

\textcolor{red}{TODO: include some overall conclusions (e.g., reasons of the success sklearn)}

Although it is coming close to a 1.0 release, \sklearn is by no means finished.
At the time, it does not support many classical machine learning tasks. Among
others, missing features include neural networks, ensemble meta-estimators for
bagging or subsampling strategies, or missing value completion algorithms.
Topics like structured prediction or reinforcement learning are however
considered off-topic for the project, since they would require quite different
data representations and API.

At a lower-level, parallel processing is a potential point of improvement.
Some estimators in \sklearn are already able to leverage multi-core processors,
but only in a coarse-grained fashion.
At present, parallel processing is difficult to accomplish in the Python environment;
\sklearn targets the main implementation, CPython,
which cannot execute Python code on multiple CPUs simultaneously.
It follows that any parallel task decomposition must either be done
inside Cython modules,
or at a level high enough to warrant the overhead
of creating multiple OS-level processes,
and the ensuing inter-process communication.
Parallel grid search is an example of the latter approach
which has already been implemented.
Recent versions of Cython include support for the OpenMP standard
\citep{dagum1998openmp},
which is a viable candidate technology
for more fine-grained multicore support in \sklearn.

\subsection*{Acknowledgements}

The authors and contributors acknowledge active support from INRIA. Past and
present sponsors of the project also include Google -- for funding code
scholarships (\textit{Google Summer of Code}) --, the Python Software Fundation and
Tinyclues -- for funding coding sprints.

\bibliographystyle{plainnat}
\DeclareRobustCommand{\VAN}[3]{#3}
\bibliography{paper}

\end{document}
