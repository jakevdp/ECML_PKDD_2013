\documentclass[twocolumn]{article}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{authblk}
\usepackage{dblfloatfix}
%\usepackage{hyperref}
\usepackage{minted}
\usepackage[authoryear,round]{natbib}
\usepackage{xcolor}
\usepackage{xspace}
\usepackage{url}

\definecolor{rulecolor}{rgb}{0.80,0.80,0.80}
\newminted{python}{frame=single,rulecolor=\color{rulecolor}}

% tt font with bold support
%\renewcommand{\ttdefault}{pcr}

\pagestyle{headings}

\newcommand{\sklearn}{\textit{scikit-learn}\xspace}

\title{API design for machine learning software: experiences from the
scikit-learn project}

\author[1]{Lars~Buitinck}
\author[2]{Gilles~Louppe}
\author[3]{Mathieu~Blondel}
\author[4]{Fabian~Pedregosa}
\author[5]{Andreas~C.~Müller}
\author[4]{Olivier~Grisel}
\author[6]{Vlad~Niculae}
\author[7]{Peter~Prettenhofer}
\author[4,8]{Alexandre~Gramfort}
\author[4]{Jaques~Grobler}
\author[9]{Robert~Layton}
\author[10]{Jake~Vanderplas}
\author[2]{Arnaud~Joly}
\author[11]{Brian Holt}
\author[4]{Gaël~Varoquaux}

\affil[1]{Netherlands eScience Center}
\affil[2]{University of Liège}
\affil[3]{Kobe University}
\affil[4]{Parietal, INRIA Saclay}
\affil[5]{University of Bonn}
\affil[6]{University of Bucharest}
\affil[7]{Ciuvo GmbH}
\affil[8]{Institut Mines-Telecom, Telecom ParisTech, CNRS LTCI}
\affil[9]{University of Ballarat}
\affil[10]{University of Washington}
\affil[11]{Samsung Electronics Research Institute}

\date{}

% Dutch name sorting hack as per http://tex.stackexchange.com/a/40750/2806
\DeclareRobustCommand{\VAN}[3]{#2}

\begin{document}

\maketitle

\sklearn is a machine learning library for the Python language.\footnote{\url{http://scikit-learn.org}}
It is designed to be simple and efficient, accessible to
non-experts, and reusable in various contexts. In this article, we present and
discuss the uniform application programming interface shared
by all learning and processing units in the library,
and the way it facilitates composition and reusability. We also comment on
implementation details specific to the Python ecosystem
and analyze potential obstacles faced by users and developers of the library.

\section{Introduction}

Many science and engineering disciplines are trying to make sense
of ever-increasing volumes of information.
Machine learning (ML) promises to alleviate this problem
by automating the process of finding patterns in the rising sea of data,
but ML tools can be hard to use
and in particular, hard to combine
when multiple algorithms are needed for a task.

The \sklearn project provides a comprehensive suite of machine learning tools,
packaged as a Python library.
It extends this general-purpose language with machine learning operations:
learning algorithms, preprocessing tools, model selection procedures
and a composition mechanism to produce complex ML workflows.
The ambition of the project is to provide efficient implementations
of well-established algorithms within a
programming environment that is accessible to non-experts and
reusable in various scientific areas.
The library is distributed under the simplified BSD license
to encourage its use in both academia and industry.

\sklearn is designed to tie in with the set of numeric
and scientific packages centered around the NumPy and SciPy libraries. NumPy
\citep{vanderwalt2011} augments Python with a contiguous numeric array datatype
and fast array computing primitives,
while SciPy \citep{varoquaux2013scipy} extends it
further with common numerical operations, either by implementing
these in Python/NumPy or by wrapping existing C/C{}\verb!++!/Fortran code.
The name ``scikit'' derives from (domain-specific) ``SciPy toolkit''
and is shared with \textit{scikit-image}.\footnote{\url{http://scikit-image.org}}
IPython \citep{perez2007ipython} and Matplotlib \citep{hunter2007matplotlib}
complement SciPy to provide a \textsc{matlab}-esque working environment
suited for scientific use.

Since 2007, \sklearn has been developed by more than a dozen
core developers, mostly researchers in various fields
(a.o.\ computer science, neuroscience, astrophysics). It also benefits
from occasional contributors proposing small bugfixes or
improvements. Development proceeds on GitHub\footnote{\url{https://github.com/scikit-learn}},
a platform which greatly facilitates this kind of
collaboration. Because of the large number of developers, emphasis is
put on keeping the project maintainable. In particular, code must follow
specific quality guidelines, such as style consistency and unit-test coverage.
Documentation and examples are required for all features,
and major changes must pass code review by at least two
developers not involved in the implementation of the proposed change.

\sklearn's popularity can be gauged from various indicators such as the hundreds
of citations in scientific publications, successes in various machine learning
challenges (e.g., Kaggle), and statistics derived from our
repositories and mailing list.  At the time of writing, the project is watched
by 1365 people and forked 693 times on GitHub; the mailing list receives more
than 300 mails per month; version control logs
% ddaa494c116e3c16bf032003c5cccbed851733d2
show 183 unique contributors to the codebase and the online documentation
receives 37,000 unique visitors and 295,000 pageviews per month.

\citet{pedregosa2011} briefly presented \sklearn and
benchmarked it against several competitors.
In this paper, we present an
in-depth analysis of design choices made when building the library,
detailing how we organized and operationalized
common machine learning concepts.
We first present in section~\ref{sec:core-api} the central application
programming interface (API) and then describe, in section~\ref{sec:advanced-api},
advanced API mechanisms built on the core interface.
Section~\ref{sec:implementation} briefly describes the implementation.
Section~\ref{sec:comparison} then
compares \sklearn to other major projects in terms of API\@.
Section~\ref{sec:future_work} outlines some of the objectives for
a \sklearn 1.0 release.
We conclude by summarizing the major points of this paper in
section~\ref{sec:conclusions}.

\section{Core API}

\label{sec:core-api}

All objects in \sklearn share a uniform basic API consisting of three
complementary interfaces: an \textit{estimator} interface for building and
fitting models, a \textit{predictor} interface for making predictions and a
\textit{transformer} interface for converting data. In this section, we describe
these three interfaces, after reviewing our general principles and data
representation choices.

\subsection{General principles}

As much as possible, \sklearn has been designed in such a way
as to avoid the proliferation of framework code.
We adopt simple conventions and limit the number of methods
an object must implement to a minimum.
The API is designed to adhere to the following principles:

\begin{description}
  \item[Consistency.]
       All objects (basic or composite) share a consistent interface composed of
       a limited set of methods. This interface is documented in a consistent
       manner for all objects.
  \item[Inspection.]
       Constructor parameters and parameter values determined by learning
       algorithms are stored and exposed as public attributes.
  \item[Non-proliferation of classes.]
       Learning algorithms are the only objects to be represented using custom classes.
       Datasets are represented as NumPy arrays or SciPy sparse matrices.
       Hyper-parameter names and values are represented as standard
       Python strings or numbers whenever possible.
       This keeps \sklearn easy to use and easy to combine with other libraries.
  \item[Composition.]
       Many machine learning tasks are expressible
       as sequences or combinations of transformations to data.
       Some learning algorithms are also naturally viewed
       as meta-algorithms parametrized on other algorithms.
       Whenever feasible, such algorithms are implemented and composed from
       existing building blocks.
  \item[Sensible defaults.]
       Whenever an operation requires a user-defined parameter,
       an appropriate default value is defined by the library.
       The default value should cause the operation to be performed
       in a sensible way (giving a baseline solution for the task at hand).
\end{description}

\subsection{Data representation}
\label{sec:arrays}

Machine learning revolves around data,
so good datastructures are paramount to designing software for it.
In most tasks, data is modeled by a set of $d$ numerical variables,
so that a single \textit{sample} (instance, observation, event)
is a vector $x \in \mathbb{R}^d$.
A collection of such samples is naturally regarded
as the rows in a matrix $X$ of size $n \times d$.
In the common case of supervised learning (classification, regression),
we have an additional vector $y$ of length $n$ at training time
and want an algorithm to produce such a $y$ for new data.

\sklearn's data representation is kept as close as possible
to this matrix formulation: datasets are encoded as
two-dimensional NumPy arrays or SciPy sparse matrices,
while target vectors are ``flat'' arrays of numbers or strings.
While these may seem rather unsophisticated when
compared to more object-oriented constructs, such as the ones used by
Weka \citep{hall2009weka},
they allow us to rely on efficient NumPy and SciPy vector operations
while keeping the code ``close to the textbook.''
Given the pervasiveness of NumPy and SciPy in many other scientific Python
packages, many scientific users of Python
will already be familiar with these data structures,
and a collection of available data loading and conversion tools
facilitate interoperability.
For tasks where the inputs are text files or semi-structured
objects, we provide \textit{vectorizer} objects that efficiently convert such
data to the NumPy or SciPy formats.

The public interface is oriented towards processing a batch of samples,
rather than a single sample, in each API call. While classification
and regression algorithms can make predictions for single samples,
\sklearn objects are not optimized for this use case. (The online learning
algorithms in the library are intended to take mini-batches.) Batch processing makes
optimal use of NumPy and SciPy by preventing the overhead inherent to Python
function calls or due to per-element dynamic type checking. Although this might
seem to be an artifact of the Python language, and therefore an implementation
detail that leaks into the API, we argue that APIs should be designed so as not
to tie a library to a suboptimal implementation strategy.
Batch processing also enables fast implementations in lower-level languages, where memory
hierarchy effects and the possibility of internal parallelization come into
play.


\subsection{Estimators}
\label{sec:estimators}

The \textit{estimator} interface is at the core of the
library. It defines instantiation mechanisms of objects and exposes a
\texttt{fit} method for learning a model from training data.  All supervised and
unsupervised learning algorithms (e.g., for classification, regression or
clustering) are offered as objects implementing this interface. Machine
learning tasks like feature extraction and selection are also provided as estimators.

Estimator initialization and actual learning are strictly separated,
in a way that is similar to partial function application:
an estimator is initialized from a set of named hyper-parameter values
(e.g., the $C$ constant in SVMs)
and can be considered a function
that maps these values to actual learning algorithms.
The constructor does not see any actual data.
All it does is attach the given parameters to the object.
For the sake of model inspection, hyper-parameters are set as public attributes,
which is especially important in model selection settings.
Default hyper-parameter values are provided for all built-in estimators.
These defaults are set to be relevant in many common
situations in order to make estimators effective \textit{out-of-box}.

Actual learning is performed by the \texttt{fit} method. This method is called
with training data (e.g., supplied as two arrays \texttt{X\_train} and
\texttt{y\_train} in supervised learning estimators). Its task is to run a
learning algorithm and to determine model-specific parameters from the training
data and set these as attributes on the estimator object. As a convention, the
parameters learned by an estimator are exposed as public attributes with names
suffixed with a trailing underscore (e.g., \texttt{coef\_} for the
learned coefficients of a linear model),
again to facilitate model inspection.
In the partial application view,
\texttt{fit} is a function from data to a model of that data.
It always returns the estimator object it was called on,
which now serves as a model of its input and can be used to perform predictions or transformations of input data.

The choice to let a single object serve dual purpose as
estimator and model has been driven by usability and technical considerations.
Having two coupled instances
(an estimator object used as a factory, and a model object produced by the estimator)
makes a library harder to learn and use.
From the developer point of view, decoupling
estimators from models would create parallel class hierarchies and increases the
overall maintenance burden.
A good reason for decoupling would be to make it possible to ship a model
to a new environment where the full library cannot be installed.
However, our inspectable setup where model parameters are documented,
public attributes, and prediction formulas follow standard textbooks,
goes a long way in solving this problem.

To illustrate the initialize-fit sequence,
let us consider a supervised learning task using logistic regression (LR).
Given the API defined above, solving this problem is as simple as the following
example.
\begin{pythoncode}
from sklearn.linear_model import LogisticRegression
clf = LogisticRegression(penalty="l1")
clf.fit(X_train, y_train)
\end{pythoncode}
In this snippet, a \texttt{LogisticRegression} estimator is first initialized by
setting the \texttt{penalty} hyper-parameter to \texttt{"l1"} for
$\ell_1$ regularization. Other hyper-parameters (such as \texttt{C},
the strength of the regularization) are not explicitly given and
thus set to the default values. Upon calling \texttt{fit}, a model is
learned from the training arrays \texttt{X\_train} and \texttt{y\_train},
and stored on the object for later use.
%% (In this example, \texttt{fit} is called solely for its side effects.)
Since all estimators share the same interface, using a different learning algorithm is
as simple as replacing the constructor (the class name);
to build a random forest on
the same data, one would simply replace
\texttt{LogisticRegression(penalty="l1")} in the snippet above by
\texttt{RandomForestClassifier()}.

In \sklearn, classical learning algorithms are not the only objects to be
implemented as estimators. For example, preprocessing routines (e.g., scaling of
features) or feature extraction techniques (e.g., vectorization of text
documents) also implement the \textit{estimator} interface. Even stateless
processing steps, that do not require the \texttt{fit} method to
perform useful work, implement the estimator interface. As we will illustrate
in the next sections, this design pattern is indeed of prime importance for
consistency, composition and model selection reasons.

\subsection{Predictors}

The \textit{predictor} interface extends the notion of an estimator
by adding a \texttt{predict}
method that takes an array \texttt{X\_test} and produces
predictions for \texttt{X\_test}, based on the learned parameters of the
estimator (we call the input to \texttt{predict} ``\texttt{X\_test}'' in order
to emphasize that \texttt{predict} generalizes to new data). In the case of
supervised learning estimators, this method returns the predicted
labels or values computed by the model:
\begin{pythoncode}
y_pred = clf.predict(X_test)
\end{pythoncode}

Some unsupervised learning estimators may also implement the \texttt{predict}
interface. The code below fits a $k$-means model with $k=10$ on
training data \texttt{X\_train}, and then uses the  \texttt{predict} method to
obtain cluster labels for unseen data \texttt{X\_test}.
\begin{pythoncode}
from sklearn.cluster import KMeans
km = KMeans(n_clusters=10)
km.fit(X_train)
clust_pred = km.predict(X_test)
\end{pythoncode}

Apart from \texttt{predict}, predictors may also implement methods
that quantify the confidence of predictions. In the case of
linear models, the \texttt{decision\_function} method returns
the distance of samples to the separating hyperplane. Some
predictors also provide a \texttt{predict\_proba} method which returns
class probabilities.

Finally, supervised predictors provide a \texttt{score} function to assess their
performance on a batch of input data. This method
takes as input arrays \texttt{X\_test} and \texttt{y\_test} and typically
computes the coefficient of determination between \texttt{y\_test} and
\texttt{predict(X\_test)} in regression, or the accuracy
in classification.
The only requirement is that the \texttt{score} method return a value
that quantifies the quality of its predictions (the higher, the better).
An unsupervised estimator may also expose a \texttt{score} function
to compute, for instance, data likelihood under its model.

\subsection{Transformers}

Since it is common to modify or filter data before feeding it to a learning
algorithm, some estimators in the library implement a \textit{transformer}
interface which defines a \texttt{transform} method. It takes as input some new
data \texttt{X\_test} and yields as output a transformed version of
\texttt{X\_test}. Preprocessing, feature selection, feature extraction and dimensionality reduction
algorithms are all provided as transformers.  In our example,
to standardize the input \texttt{X\_train} to zero mean and unit variance
before fitting the LR estimator, one would write:
\begin{pythoncode}
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X_train)
X_train = scaler.transform(X_train)
\end{pythoncode}
Of course, in practice, it is important to apply the same preprocessing to the
test data \texttt{X\_test}. Since a \texttt{StandardScaler} estimator stores the
mean and standard deviation that it computed for the training set, transforming
an unseen test set \texttt{X\_test} maps it into the appropriate region of
feature space:
\begin{pythoncode}
X_test = scaler.transform(X_test)
\end{pythoncode}
Transformers also include a variety of learning algorithms, such as
dimensionality reduction (PCA, manifold learning), kernel approximation,
and other mappings from one feature space to another.

Every transformer allows \texttt{fit(X).transform(X)}
to be written as \texttt{fit\_transform(X)}.
This combined operation prevents repeated computations,
either by skipping only an input validation step,
or by using a more efficient algorithm for the transformation.
In the same spirit, clustering algorithms provide a \texttt{fit\_predict}
that is equivalent to \texttt{fit} followed by \texttt{predict},
returning cluster labels for the training samples.


\section{Advanced API}

\label{sec:advanced-api}

Building on the core interface introduced in the previous section, we now
present advanced API mechanisms for building meta-estimators,
composing complex estimators and selecting models. We also discuss design
choices allowing for easy usage and extension of \sklearn.

\subsection{Meta-estimators}

Some learning algorithms are expressed naturally
as meta-algorithms parametrized on simpler ones.
Examples include ensemble methods which
build and combine several simpler models (boosting, bagging, random forests),
or multiclass classification schemes which combine multiple binary classifiers.
In \sklearn, such algorithms are implemented as \textit{meta-estimators}.
They take as input an estimator which is used
internally for learning and making predictions.
All meta-estimators implement the regular estimator interface.
% XXX bagging example here

Since meta-estimators require users to construct nested objects,
the decision to implement a meta-estimator
rather than integrate its functionality
into existing classes
is always based on a trade-off between generality and ease of use.

\subsection{Pipelines and feature unions}

A distinguishing feature of the \sklearn API is its ability to
compose new estimators from several base estimators. Composition mechanisms can
be used to combine typical machine learning workflows into a single object which
is itself an estimator, and can be employed wherever usual estimators can be used.
In particular, \sklearn's model selection routines
can be applied to composite estimators, allowing global optimization
of all parameters in a complex workflow.
Composition of estimators can be done in two
ways: either sequentially through \texttt{Pipeline} objects, or in a parallel
fashion through \texttt{FeatureUnion} objects.

\texttt{Pipeline} objects chain multiple estimators into a single one. This is
useful since a machine learning workflow typically involves a fixed sequence of
processing steps (e.g., feature extraction, dimensionality reduction, learning
and making predictions), many of which perform some kind of learning.
A sequence of $N$ such steps can be combined into a
\texttt{Pipeline} if the first $N-1$ steps are transformers; the last can be
either a predictor, a transformer or both.

Conceptually, fitting a pipeline to
a training set amounts to the following recursive procedure: i) when only one
step remains, call its \texttt{fit} method; ii) otherwise, \texttt{fit} the
first step, use it to \texttt{transform} the training set and \texttt{fit} the
rest of the pipeline with the transformed data. The pipeline exposes all the
methods the last estimator in the pipe exposes. That is, if the last estimator
is a predictor, the pipeline can itself be used as a predictor. If the last
estimator is a transformer, then the pipeline is itself a transformer.

\texttt{FeatureUnion} objects combine multiple transformers into a single one
that concatenates their outputs. A union of two transformers that
produce outputs with (resp.) $d_1$ and $d_2$ features, is
a transformer that produces outputs with $d_1 + d_2$ features.
This generalizes in the obvious way to more than two transformers.
Calling \texttt{fit} on the union is the same as calling \texttt{fit}
independently on each of the transformers and then joining their outputs.

\texttt{Pipeline} and \texttt{FeatureUnion} can be
combined to create complex and nested workflows.
Figure~\ref{pca-union} illustrates how to create a complex estimator
that computes both linear PCA and kernel PCA features on \texttt{X\_train}
(through a \texttt{FeatureUnion}),
and feeds those to an $\ell_2$-regularized logistic regression model.

\begin{figure*}[t]
\begin{pythoncode}
from sklearn.pipeline import FeatureUnion, Pipeline
from sklearn.decomposition import PCA, KernelPCA

union = FeatureUnion([("pca", PCA()),
                      ("kpca", KernelPCA(kernel="rbf"))])
Pipeline([("feat_union", union),
          ("log_reg", LogisticRegression(penalty="l2"))
         ]).fit(X_train, y_train)
\end{pythoncode}
\caption{A union of linear PCA and kernel PCA for feature extraction.}
\label{pca-union}
\end{figure*}

\subsection{Model selection}

As introduced in Section~\ref{sec:estimators}, hyper-parameters set in the
constructor of an estimator
determine the behavior of the learning algorithm
and hence the performance of the resulting model on unseen data.
The problem of \textit{model selection} is therefore to find, within
some hyper-parameter space, the best combination of hyper-parameters, with
respect to some user-specified criterion. For example, a decision
tree with too small a value for the maximal tree depth
parameter will tend to underfit, while too large a value will make it overfit.

In \sklearn, model selection is supported by the meta-estimators
\texttt{GridSearchCV} and \texttt{RandomizedSearchCV}.  These take as input an
estimator (basic or composite), whose hyper-parameters must be optimized, and a
set of hyperparameter settings to search through.
This set is represented as a mapping of parameter names
to a set of discrete choices in the case of grid search,
which exhaustively enumerates the ``grid'' (cartesian product)
of complete parameter combinations.
Randomized search is a smarter algorithm
that avoids the combinatorial explosion in grid search
by sampling a fixed number of times from its parameter distributions
\citep{bergstra2012}.

Optionally, the model selection algorithms
also take a cross-validation scheme and a score function.  \sklearn provides
various such schemes, including $k$-fold (default),
stratified $k$-fold and leave-one-out.
The score function used by default is the estimator's \texttt{score} method,
but the library provides a variety of
alternatives that the user can choose from,
including accuracy, AUC and $F_1$ score for classification,
$R^2$ score and mean squared error for regression.

For each hyper-parameter combination and each train/validation split
generated by the cross-validation scheme, \texttt{GridSearchCV}
and \texttt{RandomizedSearchCV} fit their base estimator on the training set and
evaluate its performance on the validation set.  In the end, the best performing
model on average is retained and exposed as the public attribute
\texttt{best\_estimator\_} (optionally re-fit on the whole training set),
but \texttt{GridSearchCV} also has a \texttt{predict} method,
which delegates to the best model's \texttt{predict},
so the CV object is itself an estimator and a predictor.

\begin{figure*}[t]
\begin{pythoncode}
from sklearn.grid_search import GridSearchCV
from sklearn.svm import SVC

param_grid = [
  {"kernel": ["linear"], "C": [1, 10, 100, 1000]},
  {"kernel": ["rbf"], "C": [1, 10, 100, 1000],
   "gamma": [0.001, 0.0001]}
]
clf = GridSearchCV(SVC(), param_grid, scoring="f1", cv=10)
clf.fit(X_train, y_train)
\end{pythoncode}
\caption{Optimizing the hyper-parameters of an SVM for $F_1$ score.}
\label{svm-gridsearch}
\end{figure*}

The code in Figure~\ref{svm-gridsearch} shows how to find
hyper-parameter settings for an SVM classifier (SVC)
that maximize $F_1$ score
through 10-fold cross-validation on the training set.
Two distinct hyper-parameter grids are
considered for the linear and radial basis function (RBF) kernels;
an SVM with a linear kernel accepts a $\gamma$ parameter, but ignores it,
so using a single parameter grid would waste computing time
trying out effectively equivalent settings.

\subsection{Extending scikit-learn}

The Python principle of \textit{duck typing} is exploited
throughout the codebase. This means that estimators are defined by interface,
not by inheritance, where the interface is entirely implicit
as far as the programming language is concerned.
Duck typing allows both for extensibility and
flexibility: as long as an estimator follows the API and conventions
outlined in Section~\ref{sec:core-api}, then it can be used in lieu of a
built-in estimator (e.g., in pipelines and grid search)
and developers are not forced to inherit from any \sklearn class.

In other places of the library, in particular in the vectorization code
for unstructured input, the toolkit is also designed to be
extensible. Here, estimators provide hooks for user-defined code: objects or
functions that follow a specific API can be given as arguments at vectorizer
construction time. The library then calls into this code,
communicating with it using objects of standard Python/NumPy types,
and keeping it agnostic of the \sklearn class hierarchy.

Our rule of thumb is that user code should not be tied to \sklearn---which is a
\textit{library}, not a framework. We try to avoid a
common problem with object-oriented design: that users wanting a
``banana'' instead get ``a gorilla holding the banana and the entire jungle''
(J.~Armstrong, cited by \citealp[p.~213]{seibel2009coders}).
Programs using \sklearn should not be intimately tied to it,
so that their code can be reused or in other contexts.


\section{Implementation}
\label{sec:implementation}

Our implementation guidelines emphasize writing efficient but readable code.
In particular, we focus on making the codebase maintainable and
understandable in order to favor external contributions. Whenever practical,
algorithms implemented in \sklearn are written in Python,
using NumPy vector operations for numerical work.
This allows the code to remain concise, readable and efficient.
For critical algorithms that cannot be easily and efficiently
expressed as NumPy operations, we rely on Cython \citep{behnel2011cython}
to achieve competitive performance and scalability. Cython is a
compiled programming language that extends Python with static typing. It
produces efficient C extension modules that are importable from the
Python run-time system. Examples of Cython code in \sklearn are
stochastic gradient descent for linear models, some clustering
algorithms, and decision trees.

To facilitate the installation of \sklearn,
the set of external dependencies is kept to a bare minimum:
only Python, NumPy and SciPy are required for a functioning installation.
Binary distributions of these are available for the major platforms.
Visualization functionality uses Matplotlib \citep{hunter2007matplotlib}
and Graphviz \citep{gansner2000},
but neither is required to perform machine learning or prediction.
When feasible, external libraries are integrated into the codebase.
In particular, \sklearn includes modified versions of \textsf{LIBSVM} and \textsf{LIBLINEAR}
\citep{chang2011libsvm,fan2008}, both written in C{}\verb!++!
and wrapped using Cython.

\section{Related software}
\label{sec:comparison}

\sklearn competes with a many machine learning packages,
both open source and proprietary.
Some of those, including Weka~\citep{hall2009weka} or
Orange~\citep{Demsar2004}, offer APIs but actually focus on the use of a graphical user interface (GUI)
which allows novices to easily apply machine learning algorithms. By
contrast, the target audience of \sklearn is capable of programming, and
therefore we focus on developing a usable and consistent API, rather than expend
effort into creating a GUI\@. In addition, while GUIs are useful tools, they
sometimes make reproducibility difficult in the case of complex workflows
(although those packages usually have developed a GUI for managing complex
tasks).

Other machine learning packages,
such as SofiaML\footnote{\url{https://code.google.com/p/sofia-ml}}
and Vowpal~Wabbit,\footnote{\url{http://hunch.net/~vw}}
are often designed as command-line tools
and may not offer any kind of API\@.
While this has the advantage that users are not tied
to a particular programming language,
they will find that they still need programming to process input/output,
and will do so in a variety of languages.
By contrast, \sklearn allows users to implement that entire workflow
in a single program, written in a single language.
This also makes it easier for researchers and developers
to exchange and collaborate on software, as dependencies and setup are kept to a
minimum.

Similar benefits hold in the case of specialized languages
for numeric and statistical programming
such as \textsc{matlab} and R \citep{r}.
In comparison to these, though, Python has the distinct advantage
that it is a \textit{general purpose} language,
while NumPy and SciPy extend it with functionality
similar to that offered by \textsc{matlab} and R.
Python has strong language and standard library support for such tasks as
string/text processing, interprocess communication, networking
and many of the other auxiliary tasks that machine learning programs
(whether academic or commercial) routinely need to perform.
While support for many of these tasks is improving in languages such as
\textsc{matlab} and R, they still lag behind Python in their general purpose
applicability.
In many applications of machine learning, these tasks, such as data access,
data preprocessing and reporting, can take more work than applying
the actual learning algorithm.

Within the realm of Python, two packages deserve mention.
The Pylearn2 library \citep{goodfellow2013pylearn2}
is similar to \sklearn in that aims to facilitate reproducible results
and model inspection.
However, it is designed for the audience of machine learning researchers,
rather than practitioners in other fields.
This leads to a modular toolkit,
where each element of an algorithm can be tweaked or replaced as needed.
This provides more control for the user,
at the cost of more math to be mastered.

Finally, the Gensim topic modeling toolkit \citep{rehurek2010gensim}
exemplifies an API geared toward ``big data'' processing.
Gensim's method of dealing with large datasets is to use algorithms
that have small space complexity and can be updated online.
The API is designed around the Python concept of an ``iterable''
(supported in the language by a restricted form of co-routines called generators).
Some \sklearn estimators, notably Naïve Bayes and linear model SGD,\footnote{
  Least-squares, logistic regression and SVM models.}
have a \texttt{partial\_fit} method to learn from a stream of samples
that is divided into mini-batches (chunks).
This way of processing, as argued in Section~\ref{sec:arrays},
allows for more efficient implementation.
We make no attempt to hide this batch-oriented processing from the user,
allowing control over the amount of memory dedicated to \sklearn algorithms.

\section{Future directions}
\label{sec:future_work}
There are several directions that the \sklearn project
aims to focus on in future development.
At present, the library does not support some classical machine learning
algorithms,
including neural networks and subsampling strategies.
However, tasks like structured prediction or reinforcement learning are
considered out of scope for the project,
since they would require quite different data representations and APIs.

At a lower-level, parallel processing is a potential point of improvement.
Some estimators in \sklearn are already able to leverage multicore processors,
but only in a coarse-grained fashion.
Parallel processing is difficult to accomplish in the Python environment;
\sklearn targets the main implementation, CPython,
which cannot execute Python code on multiple CPUs simultaneously.
It follows that any parallel task decomposition must either be done
inside Cython modules,
or at a level high enough to warrant the overhead
of creating multiple OS-level processes,
and the ensuing inter-process communication.
Parallel grid search is an example of the latter approach
which has already been implemented.
Experiments are underway to implement more fine-grained multicore support.

Finally, a long-term solution for model persistence is missing.
Currently, Python's \texttt{pickle} module is recommended for serialization,
but this only offers a file format,
not a way of preserving compatibility between versions.
Also, it has security problems because its deserializer
may execute arbitrary Python code,
so models from untrusted sources cannot be safely ``unpickled''.

These API issues will be addressed in the future in preparation for
the 1.0 release of \sklearn.


\section{Conclusion}
\label{sec:conclusions}

We have discussed the \sklearn API
and the way it maps machine learning concepts and tasks
onto objects and operations in the Python programming language.
We have shown how a consistent API across the package makes \sklearn
easy to use: experimenting with different learning
algorithms can be as simple as substituting a class name.
Through composition interfaces such as pipelines and meta-estimators,
simple building blocks lead to an API which is powerful,
and can accomplish a wide variety of learning tasks
within a small amount of code.
Through duck-typing, the consistent API leads to an extensible library
where custom algorithm can be incorporated into the workflow without inheritance.

While part of the \sklearn API is necessarily Python-specific,
core concepts may be applicable to
machine learning applications and toolkits
written in other (dynamic) programming languages.
The power and extensibility of the \sklearn API is evidenced
by the large and growing user-base, its use to solve real
problems across a wide array of fields,
as well as the appearance of third-party packages
that follow \sklearn conventions. Examples of such packages include
\textit{astroML}\footnote{\url{http://astroml.org}}
\citep{vanderplas2012astroML}, a machine learning toolkit for astronomy,
and \textit{wiseRF}\footnote{\url{http://wise.io}}, a commercial random forest
implementation. The source code of the recently-proposed sparse multiclass
algorithm of \citet{mblondel-mlj2013}, released as part of the
\textit{lightning}\footnote{\url{https://github.com/mblondel/lightning}}
package, also follows the \sklearn conventions.
We encourage researchers to follow these conventions when releasing software.

\subsection*{Acknowledgments}

The authors and contributors acknowledge active support from INRIA\@. Past and
present sponsors of the project also include Google for funding
scholarships through its Summer of Code program,
the Python Software Foundation and Tinyclues for funding coding sprints.

Gilles~Louppe and Arnaud~Joly are research fellows of the Belgian
Fonds de la Recherche Scientifique (FNRS)
and acknowledge its financial support.

{\small
\bibliographystyle{abbrvnat}
\setlength{\bibsep}{1mm}
\DeclareRobustCommand{\VAN}[3]{#3}
\bibliography{paper}
}

\end{document}
