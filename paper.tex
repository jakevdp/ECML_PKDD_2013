\documentclass{llncs}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
%\usepackage{hyperref}
\usepackage[authoryear,round]{natbib}
\usepackage{xcolor}
\usepackage{xspace}

\pagestyle{headings}

\newcommand{\sklearn}{\textit{scikit-learn}\xspace}

% working title, please change
\title{Python as a machine learning language:
       experiences from the scikit-learn project}

\author{Mathieu Blondel\inst{1} \and
        Lars Buitinck\inst{2} \and
        Gilles Louppe\inst{3} \and
        Jake Vanderplas\inst{4}}

\institute{Graduate School of System Informatics, Kobe University \and
           Informatics Institute, University of Amsterdam \and
           Department of EE \& CS, University of Li√®ge \and
           Astronomy Department, University of Washington}

% Dutch name sorting hack as per http://tex.stackexchange.com/a/40750/2806
\DeclareRobustCommand{\VAN}[3]{#2}

\begin{document}

\maketitle

\begin{abstract}
The \sklearn project is an increasingly popular open source machine
learning library designed to be simple and efficient, accessible to non-experts,
and reusable in various contexts. In this paper, we present and discuss our
design choices for the application programming interface (API) of the project.
In particular, we describe the simple and elegant interface shared by all
learning and processing units in the library and then discuss its advantages in
terms of composition and reusability. The paper also comments on implementation
details specific to the Python ecosystem and analyzes obstacles faced by users
and developers of the library.
\end{abstract}

\noindent \textcolor{red}{TODO: Focus on the experience we accumulated (general remark)}\\
\textcolor{red}{TODO: Motivate our design choices.}

\section{The scikit-learn project}

The \sklearn project \citep{pedregosa2011} is an open source
machine learning library for the Python programming language. The ambition of
the project is to provide efficient and well-established machine learning tools
within a programming environment that is accessible to non-machine learning
experts and reusable in various scientific areas. In itself the project isn't
a novel domain-specific language, but a library that provides machine
learning idioms to a general-purpose high-level language. Among other things,
it includes classical learning algorithms, model selection tools and
preprocessing procedures.

\sklearn is explicitly a library,
i.e. a collection of classes (in the object-oriented sense)
that users ``import'' into Python scripts/programs.
Use of the package requires basic Python programming knowledge,
and no command-line interface, let alone a GUI,
is offered for non-programmer users.
Instead, interactive use is made possible
by the Python interactive interpreter,
and its enhanced replacement IPython \citep{perez2007ipython},
which offers a \textsc{matlab}-like working environment
specifically designed for scientific use.

From its core, the library has been designed to tie in with the set
of numeric and scientific packages centered around the NumPy and SciPy libraries.
Of these, NumPy \citep{vanderwalt2011} augments Python
with a numeric array datatype and associated array computing primitives,
while SciPy \citep{varoquaux2013scipy} extends it further
by making common numeric operations available,
either by implementing those in Python/NumPy,
or by wrapping existing C/C{}\verb!++!/Fortran implementations.
In a similar vein, \sklearn extends the general-purpose language Python
with a set of composable machine learning operations.

Started in 2009, \sklearn is developed by an international team
of a dozen of core developers, mostly researchers from various fields
(e.g., machine learning, natural language processing, or neuroscience).
The project also greatly benefits from many occasional
contributors proposing small code or documentation fixes or improvements.
Development proceeds on GitHub (\url{https://github.com/scikit-learn}),
a platform which greatly facilitates this kind of collaboration.
Because of the large number of developers,
particular emphasis is put on keeping the project maintainable.
In particular, code must follow specific
quality guidelines, such as style consistency and unit-test coverage. In
addition, complete documentation and examples are required for all
features.
Finally, additions to the code base must pass code review
by at least two developers not involved in the implementation
of the proposed addition.

Within the past few years, the project has proved valuable to an ever increasing
user base. Its popularity can be gaged from various indicators such as hundreds
of citations in scientific publications of a previous paper
\citep{pedregosa2011}, successes in various machine learning challenges
(\emph{e.g.} Kaggle), or significant statistics derived from our repositories
and mailing lists.  At the time of writing, the project is watched by 1309
people and forked 667 time on GitHub, and the mailing-list receives 300 mails
per month.
The version control logs for the development version of scikit-learn
% ddaa494c116e3c16bf032003c5cccbed851733d2
show 183 contributors to the project's source code and documentation
(after manual deduplication).

In a previous paper \citep{pedregosa2011}, \sklearn was briefly presented and
benchmarked against several competitors. In this paper, we instead focus on an
in-depth discussion and analysis of the design choices used in
building the library. First, in section \ref{sec:api}, we present the
central application programming interface (API). Then, in section
\ref{sec:implementation}, we describe the implementation trade-offs.
In section \ref{sec:weaknesses}, we analyze the limitations of the
technologies we rely on; and finally, in section \ref{sec:comparison}, we
compare \emph{scikit-learn} to other major projects in terms of API.

% % XXX the following is more of a general introduction than a project vision
% scikit-learn \citep{pedregosa2011} is an open source machine learning library
% for the Python programming language,
% intended to tie in with the set of numeric and scientific packages
% centered around the NumPy and SciPy libraries
% \Citep{vanderwalt2011, varoquaux2013scipy}.
% It is a library, rather than a novel (domain-specific) programming language,
% intended to be called from Python programs;
% at present, it offers no command line interface or GUI.

% The scikit-learn project started in 2009 and has since been developed
% by a team largely consisting of scientists
% (both from computer science and other fields).
% Its popularity can be gauged from various sources:
% % TODO: list volume on the mailing list, number of contributors,
% % number of issues opened and closed, number of forks, number of surveys filled in.
% at the time of writing,
% the popular programmer's Q~\&{}A site StackOverflow\footnote{
%   \url{http://stackoverflow.com/questions/tagged/scikit-learn}}
% % update the following figure when paper is finalized
% lists 322 scikit-learn-related questions.

% The main goal of the scikit-learn project
% is to provide high-quality implementations
% of ``classic'' machine learning algorithms
% with a consistent API.

\section{API}

\label{sec:api}

In this section, we discuss the general principles of the application
programming interface (API) of \sklearn. We first specify the
representation of data and then proceed by describing the interface shared by
all objects of the library. We then discuss the composition and reusability
advantages of that interface.

The API is designed to adhere to the following broad principles:
\begin{description}
  \item[Inspectability.]
       \sklearn objects store their constructor parameters
       as well as parameter values determined by learning algorithms
       as public attributes in a documented format.
       (The Python language does not actually distinguish
       between public and private attributes of objects,
       but by convention, any identifier
       prefixed with an underscore (\texttt{\_})
       is considered ``a non-public part of the API''; \citealp{pythontut}.)
  \item[No proliferation of classes.]
       Learning algorithms are objects of custom classes,
       but datasets and hyperparameter names and values,
       are represented as standard Python, NumPy and SciPy classes
       rather than custom ones whenever feasible.
       Custom classes adhere to Python conventions and protocols
       for constructs such as iteration over the ``folds''
       in a $k$-fold cross validation.
       This should keep \sklearn easy to use
       and easy to combine with other libraries.
       % XXX do we need to elaborate more on this, or do we expect everyone
       % to be an experienced Java/adapter pattern hater?
  \item[Composability.]
       Many machine learning tasks are expressible
       as \textit{pipelines} of transformations to data.
       Other tasks are naturally viewed
       as modifications to other tasks;
       e.g., multi-label classification
       can be reduced to multiple binary classification tasks.
       Accordingly, multi-label classification
       is implemented by a class that takes an existing classifier object
       as input and modifies its function.
  \item[Sensible defaults.]
       Whenever an operation requires a user-defined parameter,
       a default value for that parameter must be defined by the library
       if at all possible,
       and the default should cause the operation to be performed
       in a sensible way (giving a baseline solution for the task at hand).
\end{description}

\subsection{Data representation}

In most machine learning tasks, data is modeled as a set of variables.  For
example, in a classification or regression task, the goal is to find a mapping
between input variables $X_1, ..., X_p$, called features, and
some output variable $Y$. A sample is then defined as a tuple of values $([x_1,
..., x_p]^\mathrm{T}, y)$ of these variables and a dataset as a collection of
such samples.  As such, matrices of numerical values constitute a natural and
widely used representation in machine learning. In this view, each row in the
matrix then corresponds to a sample of the dataset and each column to one of the
variables of the problem.

In \sklearn, we chose a representation of data that is as close as
possible to the matrix representation: datasets are encoded as NumPy
multidimensional arrays for dense data and as SciPy sparse matrices for sparse
data. While those may seem rather unsophisticated data representations when
compared to more object-oriented constructs, such as those used by
Weka \citep{hall2009weka}, they bring the prime advantage of allowing us to rely
on all NumPy and SciPy vector operations -- often orders of magnitude faster
that the corresponding Python loops -- while allowing us at the same time to
keep the code short and readable.  This design choice has also been motivated by
the fact that, given their pervasiveness in many other scientific Python
packages, many scientific users of Python are already familiar with NumPy dense
arrays and SciPy sparse matrices, hence helping them to familiarize with the
project.  From a practical point of view, these formats also provide a bunch of
data loading and conversion tools which make them very easy to use in many
contexts. Moreover, for tasks where the inputs are text files or semi-structured
objects, we provide \textit{vectorizer} objects that efficiently convert such
data to the NumPy or SciPy formats.

% For a machine learning library, one of the most important decisions we had to
% make is how to represent data.  Rather than reinventing the wheel, we opted for
% NumPy multidimensional arrays \Citep{vanderwalt2011} for dense data and SciPy
% sparse matrices for sparse data.  While those may seem like a ``bare''
% representation of data when compared to more object-oriented representations
% (Weka is an example of this style of representation, \citealp{hall2009weka}),
% it brings the prime advantage of allowing us to rely on NumPy and SciPy's
% \textit{vector operations} which are often orders of magnitude faster
% than the corresponding Python loops,
% while at the same time keeping the code short and readable.
% Conversion to NumPy and SciPy formats is usually easy and many scientific users
% of Python will already be used to such formats, since they are pervasive in
% other scientific Python packages.  For tasks where input is likely to consist
% of text files or semi-structured objects, we provide ``vectorizers'' -- objects
% that efficiently convert such data to the NumPy or SciPy formats.

% FIXME: elaborate what is meant below
% Lars: elaborated, but I'm not sure how important this is.
% It mostly reflects experiences with OO libraries that require conversions
% between types just to pass information to a method.
%Finally, offering a custom datatype for samples would require a conversion
%\textit{in any case}, since such a datatype would be toolkit-specific
%and no program would store its data using such a type
%if it weren't from the outset designed to use scikit-learn.

% In the remainder of the paper, we use the following notation. Training data are
% denoted by \Xtr ~and test data (i.e., unseen data to which the learning
% algorithm needs to be able to generalize) by \Xte.  Following the discussion above,
% we represent \Xtr ~and \Xte ~by 2-dimensional NumPy arrays or SciPy sparse
% matrices. Using NumPy/SciPy's notation, \Xtr\texttt{[i, j]} denotes the
% $i^\textrm{th}$ training sample's $j^\textrm{th}$ feature. For supervised learning tasks,
% such as classification or regression, we denote the training and test labels by
% \ytr ~and \yte, respectively. We typically store \ytr ~and \yte ~as
% 1-dimensional NumPy arrays (\ytr\texttt{[i]} denotes the $i^\textrm{th}$ training
% sample's label value).

\subsection{Estimators, predictors and transformers}

All objects within \sklearn share a uniform common basic API
consisting of three complementary interfaces: an \textit{estimator} interface
for building and fitting objects, a \textit{predictor} interface for making
predictions and a \textit{transformer} interface for converting data.
As much as possible, our design choices have been guided so as to avoid
the proliferation of framework code. We try to adopt simple conventions and
to limit to a minimum the number of methods an object must implement.

\textbf{Estimators.} The \textit{estimator} interface is at the core of the
library. It defines instantiation mechanisms of objects and exposes a
\texttt{fit} method for learning a model from training data.  All supervised and
unsupervised learning algorithms (e.g., for classification, regression or
clustering) are offered as objects implementing that interface. Machine
learning tasks like feature extraction, feature selection or dimensionality
reduction are also provided as estimators.

The constructor of an estimator object takes as input a set of named constant
hyper-parameters. At initialization, all given parameters are attached to the
estimator instance as public attributes and serve to determine the estimator
behavior (e.g., the $C$ constant in SVMs). Parameters set as public attributes
are also used for introspection purposes, e.g., in model selection routines. To
ease their use, default hyper-parameter values are provided for all built-in
estimators. These default values are set to be relevant in most common
situations in order to make estimators as efficient as possible
\textit{out-of-box} for non-experts. Finally, the constructor of an estimator
never performs any actual learning (this is the role of the \texttt{fit} method,
see below).  Its only goal is to set the parameter values and freeze the
behavior of the estimator.

The estimator interface defines a \texttt{fit} method. This method is called
with training data (e.g., supplied as two arrays \texttt{X\_train} and
\texttt{y\_train} in supervised learning estimators). Its task is to run a
learning algorithm and to determine model- specific parameters from the training
data and set these as attributes on the estimator object. As a convention, the
parameters learned by an estimator are exposed as public attributes whose name
is suffixed with a trailing underscore (e.g., \texttt{coef\_} for the
learned coefficients of a linear model).

% Importantly, the parameters learned by an estimator
% are exposed as \textit{public} attributes on the trained object.
% This facilitates model inspection
% and makes it possible to train a model using scikit-learn,
% export it to a (standardized or custom) external data format
% and use the learned parameters in a different piece of code,
% perhaps a prediction algorithm implemented in a different language
% or one that combines the model parameters with other information
% to build a custom model.
% % XXX Hard to follow phrasing below
% Of course, it also forces the developers to think carefully
% about what might, in a more ``black box''-style toolkit,
% be considered the internals of objects,
% since authors of client code are allowed to rely
% on the names and formats of model parameters.

As an example, let us consider a supervised learning task using logistic regression.
Given the API defined above, solving this problem is as simple as:
% XXX: Gael: I think that we should break the following lines in 2: the
% 'return self' hidden in the call to fit can be confusing to the casual
% reader
\begin{verbatim}
from sklearn.linear_model import LogisticRegression
clf = LogisticRegression(penalty="l1")
clf.fit(X_train, y_train)
\end{verbatim}
In this snippet, a \texttt{LogisticRegression} estimator is first initialized by
setting the \texttt{penalty} hyper-parameter to \texttt{"l1"}, hence enabling
$\ell_1$ regularization. Other hyper-parameters (such as \texttt{C}, which
defines the strength of the regularization) are not explicitly defined and are
thus set to the default values. Upon calling \texttt{fit}, the model is then
learned from the training arrays \texttt{X\_train} and \texttt{y\_train}. Since
all estimators share the same interface, using a different learning algorithm is
as simple as replacing the constructor. For example, to build a Random Forest on
the same training data, one would simply have to replace
\texttt{LogisticRegression(penalty="l1")} in the snippet above by \\
\texttt{RandomForestClassifier()}. 

In \sklearn, classical learning algorithms are not the only objects
to be implemented as estimators. For example, preprocessing routines (e.g.,
vectorization of text documents, scaling of features, etc) or feature
extraction techniques also implement the same interface. Even those estimators
that are entirely stateless and therefore do not require the \texttt{fit} method
to perform any useful work implement the estimator interface. As we will
illustrate in the next sections, this design pattern is indeed of prime
importance for consistency, composition and parameter validation reasons.

\textbf{Predictors.} The \textit{predictor} interface defines a \texttt{predict}
method taking as input some new data \texttt{X\_test} and producing as output
predictions for \texttt{X\_test}, based on the learned parameters of the
estimator (we call the input to \texttt{predict} ``\texttt{X\_test}'' in order
to emphasize that \texttt{predict} generalizes to new data). In the case of
supervised learning estimators, this method typically returns the predicted
labels or values computed by the model.  Continuing with the previous example,
predicted labels for \texttt{X\_test} can be obtained using the following
snippet:
\begin{verbatim}
y_pred = clf.predict(X_test)
\end{verbatim}
Some unsupervised learning
estimators may however also implement that interface, as illustrated below:
\begin{verbatim}
from sklearn.cluster import KMeans
km = KMeans(n_clusters=10)
km.fit(X_train)
clust_pred = km.predict(X_test)
\end{verbatim}
In the above example, a \texttt{KMeans} estimator is first fit on the training
data \texttt{X\_train}. Then the \texttt{predict} method is called on the test
data \texttt{X\_test} in order to retrieve predicted cluster labels.
The \texttt{predict} method in \sklearn thus offers a unified way
to compute predictions with respect to unseen data.

Along with the \texttt{predict} method, predictors may also implement methods
for quantifying the certainty of a prediction. In the case of estimators
implementing linear models, the \texttt{decision\_function} method can be used
to measure the distance of the samples from the discriminating hyperplane. Some
predictors also provide a \texttt{predict\_proba} method which returns
class probabilities for the input samples \texttt{X\_test}.

Finally, predictors should also provide a \texttt{score} function to assess the
performance on some input data. In supervised learning estimators, this method
takes as inputs a dataset \texttt{X\_test} and \texttt{y\_test} and typically
computes the coefficient of determination between \texttt{y\_test} and
\texttt{predict(X\_test)} in the case of classification or the mean squared
error in the case of regression. Not however that, from an API point of view,
the only guideline is that the \texttt{score} method returns a numerical value
such that the higher the better. As such, some unsupervised learning estimators
also exposes a \texttt{score} function, computing for instance the likelihood of
the given data.

\textbf{Transformers.} Since it is common to modify or filter data before
feeding it to a learning algorithm, some estimators in the library implement a
\textit{transformer} interface which defines a \texttt{transform} method. It
takes as input some new data \texttt{X\_test} and yields as output a transformed
version of \texttt{X\_test}.
Preprocessing, feature selection and dimensionality
reduction algorithms are all provided as transformers within the library.  In
our example, if one wanted to preprocess the inputs \texttt{X\_train} before
fitting the logistic regression estimator, e.g., by standardizing the feature
values, one could thus simply write:
\begin{verbatim}
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X_train)
X_train = scaler.transform(X_train)
\end{verbatim}
Of course, in practice, it is important to apply the same preprocessing to the
test data \texttt{X\_test}. This can easily be done as follows:
\begin{verbatim}
X_test = scaler.transform(X_test)
\end{verbatim}
Transformers also include a variety of learning algorithms, such as
dimension reduction (PCA, manifold learning techniques), kernel
approximation techniques, and any other approach that can be used to
derive a modified representation of the data.

\subsection{Method chaining}

In \sklearn, the \texttt{fit} method always returns the estimator
it was called on, hence making method chaining possible.  Using this technique,
we can thus rewrite the \texttt{StandardScaler} example above in a single line:
\begin{verbatim} 
X_train = StandardScaler().fit(X_train).transform(X_train)
\end{verbatim} 
In addition, \texttt{fit(X\_train).transform(X\_train)} can further
be rewritten as \texttt{fit\_transform(X\_train)}.  Combining the \texttt{fit}
and \texttt{transform} methods into a single method \texttt{fit\_transform}
allows to share computations, which often makes a huge difference in practice.
In the same spirit, \sklearn sometimes provides the
\texttt{fit\_predict} method for combining the \texttt{fit} and \texttt{predict}
methods. This is especially useful for retrieving cluster predictions with
respect to the training data \texttt{X\_train} in clustering algorithms.

% The bulk of the scikit-learn application programming interface
% consists of so-called \textit{estimator} objects.
% All learning algorithms for classification, regression and clustering
% are offered as such objects,
% but so are feature extraction, feature selection and dimensionality reduction
% algorithms.

% Construction and application of an estimator object
% proceeds in a way that is somewhat reminiscent of partial function application.
% The constructor, \texttt{\_\_init\_\_}\footnote{
%   Strictly speaking, \texttt{\_\_init\_\_} is Python's
%   class \textit{initialization} method,
%   while \texttt{\_\_new\_\_} is the constructor.
%   The distinction is not relevant to the present description of scikit-learn.}
% takes a set of named (hyper-)parameters
% and attaches these to the estimator as public attributes.
% It never performs actual learning, nor does it even see a training set.

% The algorithmic workhorse of most estimators, and one of the few methods
% that all estimators have in common, is called \texttt{fit}.
% Invoking this method causes the estimator to learn from a training set,
% supplied as one or more arguments.
% Its task, from an API point of view,
% is to determine (via a learning algorithm) model-specific parameters
% from a training set and set these as attributes on the estimator object,
% which can then be used to transform data or make predictions.
% As in other object-oriented machine learning libraries
% (e.g.\ Weka's Java API)
% no distinction is made in the type hierarchy
% between what is variously called an estimator or a \textit{learner}
% in the machine learning literature,
% and the \textit{model} that results from applying an estimator to data.
% When the \texttt{fit} method has been called,
% an estimator object serves dual purpose as a model
% (and is referred to as such in the documentation).

% An example to clarify the partial application style
% is the following code snippet.
% If $X$ is a dataset in the format described below and $k$ is some integer,
% then the Python code to train a $k$-means clustering model on it
% can be written as
% % XXX we could use Pygments for syntax highlighting
% \begin{verbatim}
% km = KMeans(n_clusters=k).fit(X)
% \end{verbatim}
% This snippet relies on the fact that the \texttt{fit} method
% always returns the object it was called on (its ``\texttt{self}''),
% making a method chaining style possible.

% A \texttt{fit} method is present even on objects
% that perform seemingly mundane tasks such as vectorizing text documents
% for subsequent learning.
% Such classes, however, can be said to learn their vocabulary
% from the training corpus,
% and follow the conventions set out above.
% Even those estimators that are entirely stateless and therefore
% do not require a \texttt{fit} method to perform useful work,
% nevertheless have such a method for consistency and composibility,
% and to perform parameter validation.
% Examples of this latter kind of objects are the normalizing transformer,
% kernel approximation transformers
% \citep{rahimi2007random, li2010random, vedaldi2010efficient},
% % XXX is it relevant that they are exactly two? Propose s/two/the
% and two feature extraction classes that implement the ``hashing trick''
% \citep{weinberger2009}.


\subsection{Estimator composition}

A distinguishing feature of the \sklearn API is its ability to
compose new estimators from several base estimators. Composition mechanisms can
be used to combine typical machine learning workflows into a single object which
is itself an estimator. Thanks to duck typing, composed
estimators can be used wherever usual estimators can be used. In particular,
they integrate within model selection routines, allowing one to optimize at once
every step of a complex workflow. Composition of estimators can be done in two
ways: either sequentially through \texttt{Pipeline} objects or in a parallel
fashion through \texttt{FeatureUnion} objects.

\texttt{Pipeline} objects are used for chaining multiple estimators into a
single one. This is useful since a machine learning workflow typically involves
a fixed sequence of steps in processing data (e.g., feature extraction,
normalization, feature selection, learning and making predictions).
A sequence of $N$ such steps can be combined into a composite object
called a \texttt{Pipeline}.
The first $N-1$ steps must be transformers
and the last can be either a predictor or a transformer.
Fitting a pipeline to a training set amounts, conceptually,
to the following recursive procedure:
\begin{enumerate}
  \item When only one step remains, call its \texttt{fit} method.
  \item Otherwise, fit the first step,
        use it to \texttt{transform} the training set
        and fit the rest of the pipeline with the transformed data.
\end{enumerate}
The pipeline exposes all the methods the last
estimator in the pipe exposes. That is, if the last estimator is a predictor,
the pipeline can itself be used as a predictor. If the last estimator is a
transformer, then the pipeline is itself a transformer. \textcolor{red}{TODO:
Talk about nested attribute syntax.}

A \texttt{FeatureUnion} object is a transformer combining multiple transformers
into a single one that joins their outputs. From an API point of view, a
\texttt{FeatureUnion} takes as input a list of transformers. Calling
\texttt{fit} on the union is the same as calling \texttt{fit} independently on
each of the transformers. For transforming data, the base transformers are
applied in parallel and their outputs are concatenated end-to-end into a larger
output.

As an example, the following snippet illustrates how to create a complex worflow
computing both linear PCA and kernel PCA components on \texttt{X\_train}
(through a \texttt{FeatureUnion}), selecting the 100 best of those features and
then feeding them to an $\ell_2$-regularized logistic regression model:
\begin{verbatim}
from sklearn.pipeline import Pipeline
from sklearn.decomposition import PCA, KernelPCA
from sklearn.feature_selection import SelectKBest

Pipeline([("pca", FeatureUnion([('pca', PCA()),
                                ('kpca', KernelPCA(kernel="rbf"))])),
          ("feature_selection", SelectKBest(k=100)),
          ("logistic_model", LogisticRegression(penalty="l2"))
]).fit(X_train, y_train).predict(X_test)
\end{verbatim}

As the above snippet shows, \texttt{Pipeline} and \texttt{FeatureUnion} can be
combined to create complex objects. In particular, since both are themselves
estimators, they can be used to form nested workflows. \textcolor{red}{TODO:
elaborate a bit more on nested constructions.}

% A distinguishing feature of \sklearn estimators
% is that they can be composed into \texttt{Pipeline} objects.
% Such objects combine typical machine learning workflows
% (extracting features, centering/scaling/normalization,
% feature selection, making predictions)
% in a single object that is itself an estimator,
% and whose methods cause data to flow through the pipeline.
% More importantly, though, they can be used for
% systematic, automated model selection.
% This is best explained by example.

% By composing, say, a \texttt{CountVectorizer} object
% (which extracts term frequency features from text documents
% according to a bag-words model),
% a \textsf{tf--idf} transformer, a $\chi^2$ feature selector and a linear SVM,
% one obtains a pipeline object with various hyperparameters:
% whether to do stopword filtering,
% linear vs.\ logarithmic \textsf{tf}, number of features to keep,
% SVM regularization ($\ell_1$ or $\ell_2$ norm, strength $C$) a.o.
% Such a composite \texttt{Pipeline} object
% exposes all of these parameters using a special syntax;
% each estimators must be given a name,
% and if the SVM's name is, e.g., \texttt{linearsvm},
% then its $C$ parameter is exposed as \texttt{linearsvm\_\_C}.
% The pipeline's \texttt{set\_params} splits the parameter names
% on double underscores, uses the first part of the result
% to look up the estimator,
% and passes the second part to the estimator's own \texttt{set\_params}
% as a keyword argument; due to Python's dynamic parameter passing,
% the result is the same as if the user had called \texttt{set\_params}
% on the SVM themselves.
% (The double underscore syntax is not special to pipelines;
% it is also employed in ensemble estimators such as random forests
% % Is the following clear? Suggestions to replace "direct attributes" with"?
% It is chosen as a convention to avoid ambiguity, as direct
% attributes and methods of scikit-learn objects never contain
% double underscores.)

\subsection{Model selection}

\textcolor{red}{TODO: rewrite and briefly describe the API (see user guide)}\\
\textcolor{red}{TODO: describe Scorer API?}\\
\textcolor{red}{TODO: mention that GridSearchCV objects are themselves estimators}

The aforementioned \texttt{Pipeline} object
can be passed as an argument to a model selection algorithm.
Available algorithms at the time of writing include random search
\citep{bergstra2012} and ``grid search''.
The latter, which we shall use in this example,
performs an exhaustive search through a grid formed by the cartesian product
of a set of possible values for each parameter, specified by the user.
In our example, this grid might be given as:

\begin{align*}
         & \textsf{stopwords} \in \{0, 1\}                      \\
  \times & \; \textsf{tf} \in \{\textsf{linear}, \textsf{log}\} \\
  \times & \; n_\textsf{features} \in \{1000, 2000, 5000\}      \\
  \times & \; \textsf{norm} \in \{\ell_1, \ell_2\}              \\
  \times & \; C \in \{10, 100, 1000\}
\end{align*}

At each point in this grid, $k$-fold cross validation is run
to estimate the performance of the estimator according to some measure
(e.g.\ accuracy, $F_\beta$-score or a host of clustering quality metrics)
and the best performing set of hyperparameters is stored
on the \texttt{GridSearchCV} object, again, as a public attribute.

\section{Implementation}

\label{sec:implementation}

\noindent \textcolor{red}{TODO: Mention small set of dependencies.}\\
\textcolor{red}{TODO: elaborate on use of mixins and duck typing}

scikit-learn is primarily implemented in Python and Cython
\citep{behnel2011cython},
a language that extends Python with static typing
and a compiler that produces C extension modules
for the Python runtime system.
In addition, it includes (modified versions of)
the \textsf{LIBSVM} and \textsf{LIBLINEAR} libraries
used for training support vector machines
and logistic regression models \citep{chang2011libsvm, fan2008}.
These libraries are both written in C{}\verb!++!
and wrapped using Cython modules.

As much as possible is implemented in ``pure'' Python,
to avoid the compile-edit-debug cycle needed for Cython
and to keep the codebase readable for as large an audience as possible.
However, since the speed of the current Python implementation
is not sufficient for numeric programming
and NumPy's vector operations are not appropriate for all use cases
(among other factors, because they often have linear space requirements),
some of the core algorithms have to be implemented in Cython.\footnote{
  The only Python implementation currently supported by scikit-learn
  is the reference implementation, CPython.
  The alternative Python implementation PyPy \citep{bolz2009tracing}
  promises to alleviate this problem by JIT compilation,
  but does not yet interoperate with NumPy and SciPy.
}
Examples include the aforementioned $k$-means implementation,
stochastic gradient descent for linear models
and some graph-based clustering algorithms.

While object-oriented techniques and design patterns
are used throughout scikit-learn,
they mostly serve to facilitate code reuse and are not considered a project goal.
Where possible, the Python principle of ``duck typing'' is exploited
to simplify the implementation and skip the introduction of superfluous classes.
This allows for extensibility and flexibility at the same time:
user-defined estimators that follow scikit-learn conventions
should be usable in pipelines and other composite objects
without them actually inheriting from scikit-learn base classes.

\section{Weaknesses of the NumPy/SciPy environment}

\label{sec:weaknesses}

\textcolor{red}{TODO: move this into the Implementation section? These are mostly implementation considerations.}

% This needs an introductory paragraph, but the best I could come up with was:
As can be expected, the NumPy/SciPy environment is not completely flawless.

% The following hedge needs to be in here; we don't want to diss our peers
It should be noted that this section is meant
as a summary of challenges faced by scikit-learn development
and an attempt at constructive criticism.
There is considerable overlap between the communities
that develop or contribute to NumPy, SciPy, scikit-learn, and Cython,
so the issues raised here may eventually be resolved
by the scikit-learn developers themselves.

\subsection{Sparse matrix support}

SciPy's handling of sparse matrices is one source of confusion
for scikit-learn developers and users alike.
The \texttt{scipy.sparse} package offers support
for this kind of data structure,
which is crucial to achieving performance and scalability
in the face of high-dimensional inputs
(including, but not limited to, NLP applications).
However, the interface exposed by its classes
deviates from the NumPy array interface, sometimes subtly so,
sometimes entirely.  % XXX is this grammatically correct?
For example, while NumPy arrays have a variable number of dimensions,
so that vectors can be considered 1-d arrays
(without a distinction between row and column vectors),
matrices are 2-d, etc., \texttt{scipy.sparse} matrices are always 2-dimensional,
meaning that special care has to be taken when handling sparse vectors.

The development team has built up sufficient familiarity
with the sparse/array distinction to factor out common patterns
for supporting both sparse and dense input data structures in many estimators.
In other cases, though, sparse matrix support has to implemented
on a per-estimator basis.
This situation is aggravated by the lack of a C API for sparse matrices,
so that Cython modules have to implement their own version
of operations like sparse dot products
that ideally would have been handled by a library.

However, users of scikit-learn cannot always be expected
to have similar familiarity, and in some cases
(e.g., when users want to leverage feature extraction code
outside the context of a \texttt{Pipeline})
they will have to face the \texttt{scipy.sparse} interface.
This is, of course, as much an artifact of the design choice
to not implement separate classes and interfaces for samples/instances
as it a consequence of SciPy's implementation of sparse matrices.

\subsection{Parallelism}

At this point in time, support for multi-core programming in Python,
at least in the CPython reference implementation targeted by scikit-learn,
is downright weak.
The interpreter supports multithreading, but due to the ``Global Interpreter Lock'' (GIL),
no two threads in a process may at the same time execute Python code.
It follows that multithreading is not a viable solution
even two embarrassingly parallel problems,
such as fitting and evaluation multiple models in a grid search procedure.
The workaround is to use Python's multiprocessing module,
which emulates threads using multiple operating system-level processes,
but this in turn leads to high memory use and overhead
because of the copying of training and test sets
into the various processes' address spaces.\footnote{
  The fact that copying occurs at all is an artifact of NumPy's implementation;
  no attempt at implementing a multiprocessing module
  that shares NumPy arrays using copy-on-write semantics or memory mapping
  has so far been completed.}

\subsection{Python/Cython mismatch}

The Python/Cython divide is another source of potential difficulties
both for users and developers/contributors.
While Cython makes writing performance-critical code possible
in a language that is very much like Python,
doing so properly still requires some knowledge of C.
One source of trouble has been the various fixed-size integer types
that are used by the Python and NumPy C runtimes,
whose sizes are practically all platform-dependent.
Choosing the appropriate integer type from among
\texttt{int}, \texttt{size\_t}, \texttt{Py\_ssize\_t} and \texttt{npy\_intp}
can require knowledge of the Python extension API,
the NumPy C API and the C standard.
Neither Cython nor C compilers provide much in the way of static checking
for the right type,
while picking the wrong type may cause segmentation faults
when users try to use arrays that are too large for the type used
and using a dynamic type often leads to unacceptable slowdowns.

Another problem concerns the mismatch between on the one hand Python functions,
which are first-class runtime objects endowed with methods and metadata
that identify them as ``callable'',
that dynamically check their number and types of arguments,
and that can return any Python type of object,
and compiled C or Cython functions on the other hand,
which exhibit static typing.
Where it may sometimes be convenient for estimators
to take functions as arguments
(e.g., in the case of kernel machine learning,
where one might like to use a custom kernel),
but such usage is not at present possible and may never be.
Passing C or Cython-compiled functions would require an API for the kernel method
to be implemented at the C or Cython level,
only to be used from compiled code.
Passing arbitrary Python functions would be possible,
but performance can be expected to degrade significantly
because of the runtime checks needed for each invocation of the kernel function.
(The current solution in scikit-learn is to let the user pass not a kernel function,
but a precomputed Gram matrix for an entire set of objects.
The linear, polynomial and RBF kernels are treated as special cases in
the \textsf{LIBSVM}-based SVM learner, where they are implemented in C{}\verb!++!.)

\section{Comparison with other packages}

\label{sec:comparison}

\textcolor{red}{TODO: compare with weka, R, vowpal rabbit, ... ?}\\
\textcolor{red}{TODO: emphasize scikit-learn's place in the overall scipy ecosystem, and its role in spurring the development of more basic tools.}\\
\textcolor{red}{TODO: some packages start to adopt our own API.}

\section{Future work}

ToDo.

\bibliographystyle{plainnat}
\DeclareRobustCommand{\VAN}[3]{#3}
\bibliography{paper}

\end{document}
