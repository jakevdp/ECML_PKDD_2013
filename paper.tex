\documentclass{llncs}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
%\usepackage{hyperref}
\usepackage[authoryear,round]{natbib}
\usepackage{xcolor}
\usepackage{xspace}
\usepackage{minted}

\definecolor{rulecolor}{rgb}{0.80,0.80,0.80}
\newminted{python}{frame=single,rulecolor=\color{rulecolor}}

% tt font with bold support
%\renewcommand{\ttdefault}{pcr}

\pagestyle{headings}

\newcommand{\sklearn}{\textit{scikit-learn}\xspace}

\title{ API design for machine learning software: experiences from the
scikit-learn project}

\author{Lars~Buitinck~\inst{1} \and
        Gilles~Louppe~\inst{2} \and
        Mathieu~Blondel~\inst{3} \and
        Fabian~Pedregosa~\inst{4} \and
        Andreas~Mueller~\inst{5} \and
        Olivier~Grisel~\inst{6} \and
        Vlad~Niculae~\inst{7} \and
        Peter~Prettenhofer~\inst{8} \and
        Alexandre~Gramfort~\inst{4,9} \and
        Jaques~Grobler~\inst{4} \and
        Robert~Layton~\inst{10} \and
        Jake~Vanderplas~\inst{11} \and
        Arnaud~Joly~\inst{2} \and
        Brian Holt~\inst{12} \and
        Gaël~Varoquaux~\inst{4}}

\institute{Informatics Institute, University of Amsterdam \and
           University of Liège \and
           Kobe University \and
           Parietal, INRIA Saclay \and
           University of Bonn \and
           Independent consultant \and
           University of Bucharest \and
           Ciuvo GmbH \and
           Institut Mines-Telecom, Telecom ParisTech, CNRS LTCI \and
           University of Ballarat \and
           University of Washington \and
           Samsung Electronics Research Institute}

% Dutch name sorting hack as per http://tex.stackexchange.com/a/40750/2806
\DeclareRobustCommand{\VAN}[3]{#2}

\begin{document}

\maketitle

\begin{abstract}
The \sklearn project is an increasingly popular machine learning
library. Written in Python, it is designed to be simple and efficient, accessible to
non-experts, and reusable in various contexts. In this paper, we present and
discuss our design choices for the application programming interface (API) of
the project. In particular, we describe the simple and elegant interface shared
by all learning and processing units in the library and then discuss its
advantages in terms of composition and reusability. The paper also comments on
implementation details specific to the Python ecosystem and analyzes obstacles
faced by users and developers of the library.
\end{abstract}

\section{The scikit-learn project}

The \sklearn project \citep{pedregosa2011} is an open source machine learning
library for the Python programming language. The ambition of the project is to
provide efficient and well-established machine learning tools within a
programming environment that is accessible to non-machine learning experts and
reusable in various scientific areas. The project is not a novel domain-specific
language, but a library that provides machine learning idioms to a general-
purpose high-level language. Among other things, it includes classical learning
algorithms, model selection tools and preprocessing procedures. The project is
distributed under the simplified BSD license, encouraging its use in both
academic and commercial settings.

In itself, the project is a programming library, i.e. a collection of classes
and functions that users import into Python programs. Using \sklearn therefore
requires basic Python programming knowledge. No command-line interface, let
alone a graphical user interface, is offered for non-programmer users. Instead,
interactive use is made possible by the Python interactive interpreter, and its
enhanced replacement IPython \citep{perez2007ipython}, which offer a
\textsc{matlab}-like working environment specifically designed for scientific
use.

From its core, the library has been designed to tie in with the set of numeric
and scientific packages centered around the NumPy and SciPy libraries. NumPy
\citep{vanderwalt2011} augments Python with a numeric array data type and proper
array computing primitives, while SciPy \citep{varoquaux2013scipy} extends it
further by making common numerical operations available, either by implementing
these in Python/NumPy or by wrapping existing C/C{}\verb!++!/Fortran
implementations. In a similar vein, \sklearn extends Python with a set of
composable machine learning operations.

Started in 2007, \sklearn is developed by an international team of a dozen of
core developers, mostly researchers from various fields (e.g., machine learning,
natural language processing, or neuroscience). The project also benefits
from many occasional contributors proposing small code or documentation fixes or
improvements. Development proceeds on GitHub\footnote{\url{https://github.com/scikit-learn}},
a platform which greatly facilitates this kind of
collaboration. Because of the large number of developers, emphasis is
put on keeping the project maintainable. In particular, code must follow
specific quality guidelines, such as style consistency and unit-test coverage.
Complete documentation and examples are required for all features,
and major modifications must pass code review by at least two
developers not involved in the implementation of the proposed change.

Over the past few years, the project has proved valuable to an ever increasing
user base. Its popularity can be gaged from various indicators such as hundreds
of citations in scientific publications, successes in various machine learning
challenges (e.g., Kaggle), or significant statistics derived from our
repositories and mailing list.  At the time of writing, the project is watched
by 1309 people and forked 667 times on GitHub; the mailing list receives
more than 300 mails per month; and version control logs
% ddaa494c116e3c16bf032003c5cccbed851733d2
show 183 unique contributors to the codebase.

In a previous paper \citep{pedregosa2011}, \sklearn was briefly presented and
benchmarked against several competitors. In this paper, we instead focus on an
in-depth discussion and analysis of the design choices made when building the
library. We first present in section \ref{sec:core-api} the central application
programming interface (API) and then describe, in section \ref{sec:advanced-api},
advanced API mechanisms built from the core interface. Section
\ref{sec:implementation} briefly describes the implementation.
Section \ref{sec:comparison} then
compares \emph{scikit-learn} to other major projects in terms of API. Finally,
conclusions and directions of future works are included in section
\ref{sec:conclusions}.

\section{Core API}

\label{sec:core-api}

All objects within \sklearn share a uniform common basic API consisting of three
complementary interfaces: an \textit{estimator} interface for building and
fitting models, a \textit{predictor} interface for making predictions and a
\textit{transformer} interface for converting data. As much as possible, our
design choices have been guided so as to avoid the proliferation of framework
code. We try to adopt simple conventions and to limit to a minimum the number of
methods an object must implement. The API is designed to adhere to the following
broad principles:

\begin{description}
  \item[Consistency.]
       All objects (basic or composite) share a consistent interface composed of
       a limited set of methods. This interface is documented in a consistent
       manner for all objects.
  \item[Inspection.]
       Constructor parameters and parameter values determined by learning
       algorithms are stored and exposed as public attributes.
  \item[Non-proliferation of classes.]
       Learning algorithms are the only objects to be of custom classes.
       Datasets and hyper-parameter names and values are represented as standard
       Python, NumPy or SciPy classes rather than new ones whenever feasible.
       This keeps \sklearn easy to use and easy to combine with other libraries.
  \item[Composition.]
       Many machine learning tasks are expressible
       as sequences or combinations of transformations to data.
       Some learning algorithms are also naturally viewed
       as meta-algorithms parametrized on other algorithms.
       Whenever possible, such algorithms are implemented and composed from
       existing building blocks.
  \item[Sensible defaults.]
       Whenever an operation requires a user-defined parameter,
       a default and appropriate value is defined by the library.
       The default value should cause the operation to be performed
       in a sensible way (giving a baseline solution for the task at hand).
\end{description}

\subsection{Data representation}
\label{sec:arrays}

In most machine learning tasks, data is modeled as a set of variables.  For
example, in a classification or regression task, the goal is to find a mapping
between input variables $X_1, ..., X_p$, called features, and
some output variable $Y$. A sample is then defined as a tuple of values $([x_1,
..., x_p]^\mathrm{T}, y)$ of these variables and a dataset as a collection of
such samples.  As such, matrices of numerical values constitute a natural and
widely used representation in machine learning. In this view, each row in the
matrix then corresponds to a sample of the dataset and each column to one of the
variables of the problem.

In \sklearn, we chose a representation of data that is as close as
possible to the matrix representation: datasets are encoded as NumPy
multidimensional arrays for dense data and as SciPy sparse matrices for sparse
data. While these may seem rather unsophisticated data representations when
compared to more object-oriented constructs, such as the ones used by
Weka \citep{hall2009weka}, they bring the prime advantage of allowing us to rely
on efficient NumPy and SciPy vectorized operations while keeping
the code short and readable.  This design choice has also been motivated by
the fact that, given their pervasiveness in many other scientific Python
packages, many scientific users of Python are already familiar with NumPy dense
arrays and SciPy sparse matrices.
From a practical point of view, these formats also provide a bunch of
data loading and conversion tools which make them very easy to use in many
contexts. Moreover, for tasks where the inputs are text files or semi-structured
objects, we provide \textit{vectorizer} objects that efficiently convert such
data to the NumPy or SciPy formats.

For efficiency reasons, the public interface is oriented towards processing
batches of samples rather than single samples per API call. While classification
and regression algorithms can indeed make predictions for single samples,
\sklearn objects are not optimized for this use case. (The few online learning
algorithms implemented are intended to take mini-batches.) Batch processing makes
optimal use of NumPy and SciPy by preventing the overhead inherent to Python
function calls or due to per-element dynamic type checking. Although this might
seem to be an artifact of the Python language, and therefore an implementation
detail that leaks into the API, we argue that APIs should be designed so as not
to tie a library to a suboptimal implementation strategy. As such, batch
processing enable fast implementations in lower-level languages (where memory
hierarchy effects and the possibility of internal parallelization come into
play).


\subsection{Estimators}
\label{sec:estimators}

The \textit{estimator} interface is at the core of the
library. It defines instantiation mechanisms of objects and exposes a
\texttt{fit} method for learning a model from training data.  All supervised and
unsupervised learning algorithms (e.g., for classification, regression or
clustering) are offered as objects implementing that interface. Machine
learning tasks like feature extraction, feature selection or dimensionality
reduction are also provided as estimators.

Estimator initialization and actual learning are strictly separated,
in a way that is similar to partial function application:
an estimator is initialized from a set of named constant hyper-parameter values
(e.g., the $C$ constant in SVMs)
and can be considered as a function
that maps these values to actual learning algorithms.
The constructor of an estimator does not see any actual data, nor does it perform any actual learning.
All it does is attach the given parameters to the object.
For the sake of convenient model inspection, hyper-parameters are set as public attributes,
which is especially important in model selection settings.
For ease of use, default hyper-parameter values are also provided
for all built-in estimators.
These default values are set to be relevant in most common
situations in order to make estimators as effective as possible
\textit{out-of-box} for non-experts.

Actual learning is performed by the \texttt{fit} method. This method is called
with training data (e.g., supplied as two arrays \texttt{X\_train} and
\texttt{y\_train} in supervised learning estimators). Its task is to run a
learning algorithm and to determine model-specific parameters from the training
data and set these as attributes on the estimator object. As a convention, the
parameters learned by an estimator are exposed as public attributes with names
suffixed with a trailing underscore (e.g., \texttt{coef\_} for the
learned coefficients of a linear model),
again to facilitate model inspection.
In the partial application view,
\texttt{fit} is a function from data to a model of that data.
As such, it always returns the estimator object it was called on,
which now serves as a model of its input and can be used to perform predictions or transformations of input data.
The choice to let a single object serve dual purpose as estimator and model
simplifies the \sklearn class hierarchy
compared to an approach were estimators and models are separated.

To illustrate the initialize-fit sequence,
let us consider a supervised learning task using logistic regression.
Given the API defined above, solving this problem is as simple as the following
example.
\begin{pythoncode}
from sklearn.linear_model import LogisticRegression

clf = LogisticRegression(penalty="l1")
clf.fit(X_train, y_train)
\end{pythoncode}
In this snippet, a \texttt{LogisticRegression} estimator is first initialized by
setting the \texttt{penalty} hyper-parameter to \texttt{"l1"}, hence enabling
$\ell_1$ regularization. Other hyper-parameters (such as \texttt{C}, which
defines the strength of the regularization) are not explicitly defined and are
thus set to the default values. Upon calling \texttt{fit}, the model is then
learned from the training arrays \texttt{X\_train} and \texttt{y\_train},
and stored within the object for later use.
%% (In this example, \texttt{fit} is called solely for its side effects.)
Since all estimators share the same interface, using a different learning algorithm is
as simple as replacing the constructor (the class name).
For example, to build a random forest on
the same training data, one would simply have to replace
\texttt{LogisticRegression(penalty="l1")} in the snippet above by \\
\texttt{RandomForestClassifier()}.

In \sklearn, classical learning algorithms are not the only objects to be
implemented as estimators. For example, preprocessing routines (e.g., scaling of
features) or feature extraction techniques (e.g., vectorization of text
documents) also implement the same interface. Even processing steps that are
entirely stateless and therefore do not require the \texttt{fit} method to
perform any useful work implement the estimator interface. As we will illustrate
in the next sections, this design pattern is indeed of prime importance for
consistency, composition and model selection reasons.

\subsection{Predictors}

The \textit{predictor} interface extends the notion of an estimator
by adding a \texttt{predict}
method that takes an array \texttt{X\_test} and produces
predictions for \texttt{X\_test}, based on the learned parameters of the
estimator (we call the input to \texttt{predict} ``\texttt{X\_test}'' in order
to emphasize that \texttt{predict} generalizes to new data). In the case of
supervised learning estimators, this method typically returns the predicted
labels or values computed by the model.  Continuing with the previous example,
predicted labels for \texttt{X\_test} can be obtained using the following
snippet:
\begin{pythoncode}
y_pred = clf.predict(X_test)
\end{pythoncode}

Some unsupervised learning estimators may also implement the \texttt{predict}
interface. The code in the snippet below fits a $k$-means model with $k=10$ on
training data \texttt{X\_train}, and then uses the  \texttt{predict} method to
obtain cluster labels (integer indices) for unseen data \texttt{X\_test}.
\begin{pythoncode}
from sklearn.cluster import KMeans

km = KMeans(n_clusters=10)
km.fit(X_train)
clust_pred = km.predict(X_test)
\end{pythoncode}

Along with the \texttt{predict} method, predictors may also implement methods
for quantifying the certainty of a prediction. In the case of estimators
implementing linear models, the \texttt{decision\_function} method can be used
to measure the distance of the samples from the discriminating hyperplane. Some
predictors also provide a \texttt{predict\_proba} method which returns
class probabilities for the input samples \texttt{X\_test}.

Finally, predictors should also provide a \texttt{score} function to assess the
performance on some input data. In supervised learning estimators, this method
takes as inputs arrays \texttt{X\_test} and \texttt{y\_test} and typically
computes the coefficient of determination between \texttt{y\_test} and
\texttt{predict(X\_test)} in the case of classification, or the mean squared
error in the case of regression. Not however that, from an API point of view,
the only guideline is that the \texttt{score} method returns a value
that quantifies the quality of its predictions (the higher, the better).
Unsupervised learning estimators may also exposes a \texttt{score} function
to compute, for instance, the likelihood of the given data under its model.

\subsection{Transformers}

Since it is common to modify or filter data before feeding it to a learning
algorithm, some estimators in the library implement a \textit{transformer}
interface which defines a \texttt{transform} method. It takes as input some new
data \texttt{X\_test} and yields as output a transformed version of
\texttt{X\_test}. Preprocessing, feature selection and dimensionality reduction
algorithms are all provided as transformers within the library.  In our example,
if one wanted to preprocess the inputs \texttt{X\_train} before fitting the
logistic regression estimator, e.g., by standardizing the feature values, one
could thus simply write:
\begin{pythoncode}
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(X_train)
X_train = scaler.transform(X_train)
\end{pythoncode}
Of course, in practice, it is important to apply the same preprocessing to the
test data \texttt{X\_test}. Since a \texttt{StandardScaler} estimator stores the
mean and standard deviation that it computed for the training set, transforming
an unseen test set \texttt{X\_test} maps it into the appropriate region of
feature space:
\begin{pythoncode}
X_test = scaler.transform(X_test)
\end{pythoncode}
Transformers also include a variety of learning algorithms, such as
dimension reduction (PCA, manifold learning), kernel approximation,
and other mappings from one feature space to another.

Additionally, by leveraging the fact that \texttt{fit} always returns the
estimator it was called on, the \texttt{StandardScaler} example above can be
rewritten in a single line using method chaining:
\begin{pythoncode}
X_train = StandardScaler().fit(X_train).transform(X_train)
\end{pythoncode}

Furthermore, every transformer allows \texttt{fit(X\_train).transform(X\_train)}
to be written as \texttt{fit\_transform(X\_train)}.
The combined \texttt{fit\_transform} method prevents repeated computations.
Depending on the transformer,
it may skip only an input validation step,
or in fact use a more efficient algorithm for the transformation.
In the same spirit, clustering algorithms provide a
\texttt{fit\_predict} method
that is equivalent to \texttt{fit} followed by \texttt{predict},
returning cluster labels assigned to the training samples.


\section{Advanced API}

\label{sec:advanced-api}

Building on the core interface introduced in the previous section, we now
present advanced API mechanisms for building meta-estimators,
composing complex estimators and then selecting models. We also discuss design
choices allowing for an easy use and extension of \sklearn.

\subsection{Meta-estimators}

Some machine learning algorithms are expressed naturally
as meta-algorithms parametrized on simpler algorithms.
Examples include ensemble methods which
build and combine several simpler models (e.g., decision trees), or multiclass
and multilabel classification schemes which can be used to turn a binary
classifier into a multiclass or multilabel classifier. In \sklearn, such algorithms are
implemented as \textit{meta-estimators}. They take as input an existing base
estimator and use it internally for learning and making predictions.
All meta-estimators implement the regular estimator interface.

As an example, a logistic regression classifier
uses by default a one-vs.-rest scheme
for performing multiclass classification.
A different scheme can be achieved
by a meta-estimator wrapping a logistic regression estimator:
\begin{pythoncode}
from sklearn.multiclass import OneVsOneClassifier

ovo_lr = OneVsOneClassifier(LogisticRegression(penalty="l1"))
\end{pythoncode}
For learning, the \texttt{OneVsOneClassifier} object
\textit{clones} the logistic regression estimator multiple times,
resulting in a set of $\frac{K(K-1)}{2}$ estimator objects
for $K$-way classification,
all with the same settings.
For predictions, all estimators perform a binary classification and then vote to make the final decision.
The snippet exemplifies the importance
of separating object instantiation and actual learning.

Since meta-estimators require users to construct nested objects,
the decision to implement a meta-estimator
rather than fold the behavior it implements
into existing estimators classes
is always based on a trade-off between generality and ease of use.
Relating to the example just given,
all \sklearn classifiers are designed to do multiclass classification
and the use of the \texttt{multiclass} module
is only necessary in advanced use cases.

% TODO: show a grid search on this estimator

\subsection{Pipelines and feature unions}

A distinguishing feature of the \sklearn API is its ability to
compose new estimators from several base estimators. Composition mechanisms can
be used to combine typical machine learning workflows into a single object which
is itself an estimator, and can be used wherever usual estimators can be used.
In particular,
they integrate within model selection routines, allowing one to optimize at once
every step of a complex workflow. Composition of estimators can be done in two
ways: either sequentially through \texttt{Pipeline} objects or in a parallel
fashion through \texttt{FeatureUnion} objects.

\texttt{Pipeline} objects chain multiple estimators into a
single one. This is useful since a machine learning workflow typically involves
a fixed sequence of processing steps (e.g., feature extraction,
dimensionality reduction, learning and making predictions). A sequence
of $N$ such steps can be combined into a \texttt{Pipeline}
if the first $N-1$ steps are transformers; the last can
be either a predictor or a transformer. Conceptually, fitting a pipeline to a
training set amounts to the following recursive procedure: i) when only one step
remains, call its \texttt{fit} method; ii) otherwise, \texttt{fit} the first
step, use it to \texttt{transform} the training set and \texttt{fit} the rest of
the pipeline with the transformed data. The pipeline exposes all the methods the
last estimator in the pipe exposes. That is, if the last estimator is a
predictor, the pipeline can itself be used as a predictor. If the last estimator
is a transformer, then the pipeline is itself a transformer.

\texttt{FeatureUnion} objects combine multiple transformers into a single one
that concatenates their outputs. A union of two transformers that
map inputs having $d$ features to $d'$ and $d''$ features respectively is
a transformer that maps its $d$ input features to $d' + d''$ features.
This generalizes in the obvious way to more than two transformers.
In terms of API, a \texttt{FeatureUnion} takes as input a list of transformers.
Calling \texttt{fit} on the union is the same as calling \texttt{fit}
independently on each of the transformers and then joining their outputs.

\texttt{Pipeline} and \texttt{FeatureUnion} can be
combined to create complex and nested workflows.
The following snippet illustrates how to create a complex estimator
that computes both linear PCA and kernel PCA features on \texttt{X\_train}
(through a \texttt{FeatureUnion}),
selects the 10 best features in the combination according to an ANOVA test
and feeds those to an $\ell_2$-regularized logistic regression model.
\begin{pythoncode}
from sklearn.pipeline import FeatureUnion, Pipeline
from sklearn.decomposition import PCA, KernelPCA
from sklearn.feature_selection import SelectKBest

union = FeatureUnion([("pca", PCA()),
                      ("kpca", KernelPCA(kernel="rbf"))])

Pipeline([("feat_union", union),
          ("feat_sel", SelectKBest(k=10)),
          ("log_reg", LogisticRegression(penalty="l2"))
]).fit(X_train, y_train).predict(X_test)
\end{pythoncode}

\subsection{Model selection}

As introduced in Section~\ref{sec:estimators}, hyper-parameters set in the
constructor of an estimator
determine the behavior of its learning algorithm
and hence the performance of the resulting model on unseen data.
The problem of \textit{model selection} is therefore to find, within
some hyper-parameter space, the best combination of hyper-parameters, with
respect to some user-chosen criterion. For example, in the case of a decision
tree, too small values of the \texttt{max\_depth} parameter will make the tree
underfit, while too large values will make it overfit.

In \sklearn, model selection is supported in two distinct meta-estimators,
\texttt{GridSearchCV} and \texttt{RandomizedSearchCV}.  They take as inputs an
estimator (basic or composite), whose hyper-parameters must be optimized, and a
hyper-parameter grid, which defines the values to search. Optionally, they can
also take a cross-validation scheme and a score function.  The library provides
various such cross-validation schemes, including $k$-fold (default),
leave-one-out and bootstrapping.
The score function used by default is the \texttt{score} method
provided by the estimator. However, the library provides a large variety of
score functions that the user can choose from,
including accuracy, AUC, $F_1$ score (classification),
$R^2$ score and mean squared error (regression).

\texttt{GridSearchCV} and \texttt{RandomizedSearchCV} differ in the way they
search the hyper-parameter grid.  \texttt{GridSearchCV} exhaustively searches
for all hyper-parameter combinations from the grid. This scheme can be expensive
when there are many possible hyper-parameter combinations.
\texttt{RandomizedSearchCV}, on the other hand, only searches for a limited
number of combinations, randomly sampled from the grid. Such scheme has been
shown to work well in practice while considerably reducing the computational
cost \citep{bergstra2012}. For each hyper-parameter combination and each train /
validation split generated by the cross-validation scheme, \texttt{GridSearchCV}
and \texttt{RandomizedSearchCV} fit the base estimator on the training set and
evaluate its performance on the validation set.  In the end, the best performing
model on average is retained and exposed as the public attribute
\texttt{best\_estimator\_}.

The snippet below illustrates how to find
hyper-parameter settings for an SVM classifier (SVC)
that maximize $F_1$ score
through 10-fold cross-validation on the training set.
\begin{pythoncode}
from sklearn.grid_search import GridSearchCV
from sklearn.svm import SVC

param_grid = [
  {"kernel": ["linear"], "C": [1, 10, 100, 1000]},
  {"kernel": ["rbf"], "C": [1, 10, 100, 1000],
                      "gamma": [0.001, 0.0001]}
]

clf = GridSearchCV(SVC(), param_grid, scoring="f1", cv=10)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
\end{pythoncode}
Note that, in the above example, two distinct hyper-parameter grids are
considered. This is because it only makes sense to search for \texttt{gamma}
when \texttt{kernel} is set to \texttt{"rbf"}.
An SVM with a linear kernel accepts a $\gamma$ parameter,
but actually ignores it. Computing time would thus be wasted, trying out effectively
equivalent settings, if a single hyper-parameter grid was used instead.
Additionally, we see that
\texttt{GridSearchCV} has a \texttt{predict} method, like any other classifier.
This is because \texttt{GridSearchCV} and \texttt{RandomizedSearchCV} in fact
delegate the \texttt{predict}, \texttt{predict\_proba}, \texttt{transform} and
\texttt{score} methods to the best estimator
(optionally after re-fitting it on the whole training set).

\subsection{Extending scikit-learn}

To ease code reuse, simplify implementation and skip the introduction of
superfluous classes, the Python principle of \textit{duck typing} is exploited
throughout the codebase. This means that estimators are defined by interface,
not by inheritance, where the interface is entirely implicit
as far as the programming language is concerned.
Duck typing allows both for extensibility and
flexibility: as long as an estimator follows the API and conventions
outlined in Section~\ref{sec:core-api}, then it can be used in lieu of a
built-in estimator (e.g., it can be plugged into pipelines or grid search)
and external developers are not forced to inherit from any \sklearn class.

In other places of the library, in particular in the vectorization code
for unstructured inputs, the toolkit is also designed to be
extensible. Here, estimators provide hooks for user-defined code: objects or
functions that provide a specific API can be given as arguments at vectorizer
construction time. The library then calls into this code, communicating with it by passing objects of standard Python/Numpy types.
Again, such external user code can be kept agnostic of the \sklearn
class hierarchy.

Our rule of thumb is that user code should not be tied to \sklearn---which is a
\textit{library}, and not a \textit{framework}. This principle indeed avoids a
well-known problem with object-oriented design, which is that users wanting a
``banana'' should not get ``a gorilla holding the banana and the entire jungle''
(J.~Armstrong, cited by \citealp[p.~213]{seibel2009coders}).
That is, programs using \sklearn should not be intimately tied to it,
so that their code can be reused with other toolkits or in other contexts.


\section{Implementation}
\label{sec:implementation}

Our implementation guidelines emphasize writing efficient but readable code.
In particular, we focus on making the codebase easily maintainable and
understandable in order to favour external contributions. Whenever practicable,
algorithms implemented in \sklearn are written in Python,
using Numpy vector operations for numerical work.
This allows for the code to remain concise, readable and
efficient. For critical algorithms that cannot be easily and efficiently
expressed as NumPy operations, we rely on Cython \citep{behnel2011cython}
to achieve decent performance and scalability. Cython is a
compiled programming language that extends Python with static typing. It
produces efficient C extension modules that are directly importable from the
Python run-time system. Examples of algorithms written in Cython include
stochastic gradient descent for linear models, some graph-based clustering
algorithms and decision trees.

To facilitate the installation and thus adoption of \sklearn,
the set of external dependencies is kept to a bare minimum:
only Python, NumPy and SciPy are required for a functioning installation.
Binary distributions of these are available for the major platforms.
Visualisation functionality depends on Matplotlib \citep{hunter2007matplotlib}
and/or Graphviz \citep{gansner2000},
but neither is required to do machine learning.
When feasible, external libraries are integrated into the codebase.
In particular, \sklearn includes modified versions of \textsf{LIBSVM} and \textsf{LIBLINEAR}
\citep{chang2011libsvm,fan2008}, both written in C{}\verb!++!
and wrapped using Cython modules.

\section{Comparison with other packages}

\label{sec:comparison}

\textcolor{red}{TODO: compare with weka, R, vowpal rabbit, ... ?}\\
\textcolor{red}{TODO: emphasize scikit-learn's place in the overall scipy ecosystem, and its role in spurring the development of more basic tools.}\\
\textcolor{red}{TODO: some packages start to adopt our own API.}

Recent years have witnessed a rising interest in machine learning and data mining
with applications in many fields.
With this rise comes a host of machine learning packages
(both open source and proprietary) with which \sklearn competes.

In comparison to specialized languages for numeric and statistical programming
such as \textsc{matlab} and R \citep{r},
the use of Python as a host language has the distinct advantage
that it is a \textit{general purpose} language:
it has strong language and standard library support for such tasks as
string handling/text processing, interprocess communication, networking
and many of the other tasks that many machine learning programs
(whether academic or commercial)
will have to perform ``on the side''.
In addition, third-party libraries are available for many other routine tasks.

Another package that deserves mention is the Gensim topic modeling toolkit
\citep{rehurek2010gensim}.
Although its stated goal is not machine learning,
it does exemplify API design for scalable processing of ``big data''
that takes a different approach than comparable models in \sklearn.
Gensim's way of dealing with large datasets is to use algorithms
that have $O(1)$ space complexity and can be updated online.
The API is designed around the Python concept of an \textit{iterable}
(supported in the language by a restricted form of co-routines called
\textit{generators}).
While the text vectorization part of \sklearn
also uses iterables to some extent,
the emphasis is still on (mini-)batch processing,
even in the stateless, O(1) memory vectorizers
that implement the hashing trick of \citet{weinberger2009}.
This way of processing, as argued earlier in Section~\ref{sec:arrays},
reduces various forms of overhead
and makes effective use of the abundance of memory in modern computers.
We make no attempt to hide this batch-oriented processing from the user,
allowing control over the amount of memory actually dedicated
to \sklearn algorithms.


\section{Conclusions and future work}
\label{sec:conclusions}

We have discussed the scikit-learn API
and the way it maps machine learning concepts and tasks
onto objects and operations in the Python programming language.
We motivated choices and compromises made in its design.
We argue that the API is powerful, usable and extensible,
as witnessed by the appearance of third-party packages
that follow the scikit-learn conventions and tie into the toolkit.\footnote{
  E.g.\ nolearn, \url{http://pythonhosted.org/nolearn};
  this mention does not imply endorsement of this package.}
While part of the scikit-learn API is necessarily Python-specific,
core concepts may be applicable to
machine learning applications and toolkits
written in other (dynamic) programming languages.

A \sklearn~1.0 release is nearing,
but several problems must still be resolved.
At present, the library does not support some classical machine learning tasks,
including neural networks, ensemble meta-estimators for
bagging or subsampling strategies and missing value completion algorithms.
Topics like structured prediction or reinforcement learning are
considered off-topic for the project, since they would require quite different
data representations and APIs.

At a lower-level, parallel processing is a potential point of improvement.
Some estimators in \sklearn are already able to leverage multi-core processors,
but only in a coarse-grained fashion.
At present, parallel processing is difficult to accomplish in the Python environment;
\sklearn targets the main implementation, CPython,
which cannot execute Python code on multiple CPUs simultaneously.
It follows that any parallel task decomposition must either be done
inside Cython modules,
or at a level high enough to warrant the overhead
of creating multiple OS-level processes,
and the ensuing inter-process communication.
Parallel grid search is an example of the latter approach
which has already been implemented.
Recent versions of Cython include support for the OpenMP standard
\citep{dagum1998openmp},
which is a viable candidate technology
for more fine-grained multicore support in \sklearn.

Finally, a long-term solution for model persistence is missing.
Currently, Python's \texttt{pickle} module is recommended for serialization,
but this only offers a file format,
not a way of preserving compatibility between versions.
Also, it has security problems because its deserializer
may execute arbitrary Python code,
so models from untrusted sources cannot be safely ``unpickled''.

\subsection*{Acknowledgements}

The authors and contributors acknowledge active support from INRIA. Past and
present sponsors of the project also include Google for funding
scholarships through its Summer of Code program,
the Python Software Fundation and Tinyclues for funding coding sprints.

\bibliographystyle{abbrvnat}
\DeclareRobustCommand{\VAN}[3]{#3}
\bibliography{paper}

\end{document}
